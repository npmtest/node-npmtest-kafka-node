{"/home/travis/build/npmtest/node-npmtest-kafka-node/test.js":"/* istanbul instrument in package npmtest_kafka_node */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        switch (local.modeJs) {\n        // re-init local from window.local\n        case 'browser':\n            local = local.global.utility2.objectSetDefault(\n                local.global.utility2_rollup || local.global.local,\n                local.global.utility2\n            );\n            break;\n        // re-init local from example.js\n        case 'node':\n            local = (local.global.utility2_rollup || require('utility2'))\n                .requireReadme();\n            break;\n        }\n        // export local\n        local.global.local = local;\n    }());\n\n\n\n    // run shared js-env code - function\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - function\n    case 'browser':\n        break;\n\n\n\n    // run node js-env code - function\n    case 'node':\n        break;\n    }\n\n\n\n    // run shared js-env code - post-init\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - post-init\n    case 'browser':\n        local.testCase_browser_nullCase = local.testCase_browser_nullCase || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test browsers's null-case handling-behavior-behavior\n         */\n            onError(null, options);\n        };\n\n        // run tests\n        local.nop(local.modeTest &&\n            document.querySelector('#testRunButton1') &&\n            document.querySelector('#testRunButton1').click());\n        break;\n\n\n\n    // run node js-env code - post-init\n    /* istanbul ignore next */\n    case 'node':\n        local.testCase_buildApidoc_default = local.testCase_buildApidoc_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApidoc's default handling-behavior-behavior\n         */\n            options = { modulePathList: module.paths };\n            local.buildApidoc(options, onError);\n        };\n\n        local.testCase_buildApp_default = local.testCase_buildApp_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApp's default handling-behavior-behavior\n         */\n            local.testCase_buildReadme_default(options, local.onErrorThrow);\n            local.testCase_buildLib_default(options, local.onErrorThrow);\n            local.testCase_buildTest_default(options, local.onErrorThrow);\n            local.testCase_buildCustomOrg_default(options, local.onErrorThrow);\n            options = [];\n            local.buildApp(options, onError);\n        };\n\n        local.testCase_buildCustomOrg_default = local.testCase_buildCustomOrg_default ||\n            function (options, onError) {\n            /*\n             * this function will test buildCustomOrg's default handling-behavior\n             */\n                options = {};\n                local.buildCustomOrg(options, onError);\n            };\n\n        local.testCase_buildLib_default = local.testCase_buildLib_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildLib's default handling-behavior\n         */\n            options = {};\n            local.buildLib(options, onError);\n        };\n\n        local.testCase_buildReadme_default = local.testCase_buildReadme_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildReadme's default handling-behavior-behavior\n         */\n            options = {};\n            local.buildReadme(options, onError);\n        };\n\n        local.testCase_buildTest_default = local.testCase_buildTest_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildTest's default handling-behavior\n         */\n            options = {};\n            local.buildTest(options, onError);\n        };\n\n        local.testCase_webpage_default = local.testCase_webpage_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test webpage's default handling-behavior\n         */\n            options = { modeCoverageMerge: true, url: local.serverLocalHost + '?modeTest=1' };\n            local.browserTest(options, onError);\n        };\n\n        // run test-server\n        local.testRunServer(local);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-kafka-node/lib.npmtest_kafka_node.js":"/* istanbul instrument in package npmtest_kafka_node */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || local;\n        // init lib\n        local.local = local.npmtest_kafka_node = local;\n        // init exports\n        if (local.modeJs === 'browser') {\n            local.global.utility2_npmtest_kafka_node = local;\n        } else {\n            module.exports = local;\n            module.exports.__dirname = __dirname;\n            module.exports.module = module;\n        }\n    }());\n}());\n","/home/travis/build/npmtest/node-npmtest-kafka-node/example.js":"/*\nexample.js\n\nquickstart example\n\ninstruction\n    1. save this script as example.js\n    2. run the shell command:\n        $ npm install npmtest-kafka-node && PORT=8081 node example.js\n    3. play with the browser-demo on http://127.0.0.1:8081\n*/\n\n\n\n/* istanbul instrument in package npmtest_kafka_node */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || (local.modeJs === 'browser'\n            ? local.global.utility2_npmtest_kafka_node\n            : global.utility2_moduleExports);\n        // export local\n        local.global.local = local;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // post-init\n    // run browser js-env code - post-init\n    /* istanbul ignore next */\n    case 'browser':\n        local.testRunBrowser = function (event) {\n            if (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('onreset'))) {\n                // reset output\n                Array.from(\n                    document.querySelectorAll('body > .resettable')\n                ).forEach(function (element) {\n                    switch (element.tagName) {\n                    case 'INPUT':\n                    case 'TEXTAREA':\n                        element.value = '';\n                        break;\n                    default:\n                        element.textContent = '';\n                    }\n                });\n            }\n            switch (event && event.currentTarget && event.currentTarget.id) {\n            case 'testRunButton1':\n                // show tests\n                if (document.querySelector('#testReportDiv1').style.display === 'none') {\n                    document.querySelector('#testReportDiv1').style.display = 'block';\n                    document.querySelector('#testRunButton1').textContent =\n                        'hide internal test';\n                    local.modeTest = true;\n                    local.testRunDefault(local);\n                // hide tests\n                } else {\n                    document.querySelector('#testReportDiv1').style.display = 'none';\n                    document.querySelector('#testRunButton1').textContent = 'run internal test';\n                }\n                break;\n            // custom-case\n            default:\n                break;\n            }\n            if (document.querySelector('#inputTextareaEval1') && (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('oneval')))) {\n                // try to eval input-code\n                try {\n                    /*jslint evil: true*/\n                    eval(document.querySelector('#inputTextareaEval1').value);\n                } catch (errorCaught) {\n                    console.error(errorCaught);\n                }\n            }\n        };\n        // log stderr and stdout to #outputTextareaStdout1\n        ['error', 'log'].forEach(function (key) {\n            console[key + '_original'] = console[key];\n            console[key] = function () {\n                var element;\n                console[key + '_original'].apply(console, arguments);\n                element = document.querySelector('#outputTextareaStdout1');\n                if (!element) {\n                    return;\n                }\n                // append text to #outputTextareaStdout1\n                element.value += Array.from(arguments).map(function (arg) {\n                    return typeof arg === 'string'\n                        ? arg\n                        : JSON.stringify(arg, null, 4);\n                }).join(' ') + '\\n';\n                // scroll textarea to bottom\n                element.scrollTop = element.scrollHeight;\n            };\n        });\n        // init event-handling\n        ['change', 'click', 'keyup'].forEach(function (event) {\n            Array.from(document.querySelectorAll('.on' + event)).forEach(function (element) {\n                element.addEventListener(event, local.testRunBrowser);\n            });\n        });\n        // run tests\n        local.testRunBrowser();\n        break;\n\n\n\n    // run node js-env code - post-init\n    /* istanbul ignore next */\n    case 'node':\n        // export local\n        module.exports = local;\n        // require modules\n        local.fs = require('fs');\n        local.http = require('http');\n        local.url = require('url');\n        // init assets\n        local.assetsDict = local.assetsDict || {};\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.index.template.html'] = '\\\n<!doctype html>\\n\\\n<html lang=\"en\">\\n\\\n<head>\\n\\\n<meta charset=\"UTF-8\">\\n\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\\n<title>{{env.npm_package_name}} (v{{env.npm_package_version}})</title>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n    box-sizing: false,\\n\\\n    universal-selector: false\\n\\\n*/\\n\\\n* {\\n\\\n    box-sizing: border-box;\\n\\\n}\\n\\\nbody {\\n\\\n    background: #dde;\\n\\\n    font-family: Arial, Helvetica, sans-serif;\\n\\\n    margin: 2rem;\\n\\\n}\\n\\\nbody > * {\\n\\\n    margin-bottom: 1rem;\\n\\\n}\\n\\\n.utility2FooterDiv {\\n\\\n    margin-top: 20px;\\n\\\n    text-align: center;\\n\\\n}\\n\\\n</style>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n*/\\n\\\ntextarea {\\n\\\n    font-family: monospace;\\n\\\n    height: 10rem;\\n\\\n    width: 100%;\\n\\\n}\\n\\\ntextarea[readonly] {\\n\\\n    background: #ddd;\\n\\\n}\\n\\\n</style>\\n\\\n</head>\\n\\\n<body>\\n\\\n<!-- utility2-comment\\n\\\n<div id=\"ajaxProgressDiv1\" style=\"background: #d00; height: 2px; left: 0; margin: 0; padding: 0; position: fixed; top: 0; transition: background 0.5s, width 1.5s; width: 25%;\"></div>\\n\\\nutility2-comment -->\\n\\\n<h1>\\n\\\n<!-- utility2-comment\\n\\\n    <a\\n\\\n        {{#if env.npm_package_homepage}}\\n\\\n        href=\"{{env.npm_package_homepage}}\"\\n\\\n        {{/if env.npm_package_homepage}}\\n\\\n        target=\"_blank\"\\n\\\n    >\\n\\\nutility2-comment -->\\n\\\n        {{env.npm_package_name}} (v{{env.npm_package_version}})\\n\\\n<!-- utility2-comment\\n\\\n    </a>\\n\\\nutility2-comment -->\\n\\\n</h1>\\n\\\n<h3>{{env.npm_package_description}}</h3>\\n\\\n<!-- utility2-comment\\n\\\n<h4><a download href=\"assets.app.js\">download standalone app</a></h4>\\n\\\n<button class=\"onclick onreset\" id=\"testRunButton1\">run internal test</button><br>\\n\\\n<div id=\"testReportDiv1\" style=\"display: none;\"></div>\\n\\\nutility2-comment -->\\n\\\n\\n\\\n\\n\\\n\\n\\\n<label>stderr and stdout</label>\\n\\\n<textarea class=\"resettable\" id=\"outputTextareaStdout1\" readonly></textarea>\\n\\\n<!-- utility2-comment\\n\\\n{{#if isRollup}}\\n\\\n<script src=\"assets.app.js\"></script>\\n\\\n{{#unless isRollup}}\\n\\\nutility2-comment -->\\n\\\n<script src=\"assets.utility2.rollup.js\"></script>\\n\\\n<script src=\"jsonp.utility2._stateInit?callback=window.utility2._stateInit\"></script>\\n\\\n<script src=\"assets.npmtest_kafka_node.rollup.js\"></script>\\n\\\n<script src=\"assets.example.js\"></script>\\n\\\n<script src=\"assets.test.js\"></script>\\n\\\n<!-- utility2-comment\\n\\\n{{/if isRollup}}\\n\\\nutility2-comment -->\\n\\\n<div class=\"utility2FooterDiv\">\\n\\\n    [ this app was created with\\n\\\n    <a href=\"https://github.com/kaizhu256/node-utility2\" target=\"_blank\">utility2</a>\\n\\\n    ]\\n\\\n</div>\\n\\\n</body>\\n\\\n</html>\\n\\\n';\n        /* jslint-ignore-end */\n        if (local.templateRender) {\n            local.assetsDict['/'] = local.templateRender(\n                local.assetsDict['/assets.index.template.html'],\n                {\n                    env: local.objectSetDefault(local.env, {\n                        npm_package_description: 'the greatest app in the world!',\n                        npm_package_name: 'my-app',\n                        npm_package_nameAlias: 'my_app',\n                        npm_package_version: '0.0.1'\n                    })\n                }\n            );\n        } else {\n            local.assetsDict['/'] = local.assetsDict['/assets.index.template.html']\n                .replace((/\\{\\{env\\.(\\w+?)\\}\\}/g), function (match0, match1) {\n                    // jslint-hack\n                    String(match0);\n                    switch (match1) {\n                    case 'npm_package_description':\n                        return 'the greatest app in the world!';\n                    case 'npm_package_name':\n                        return 'my-app';\n                    case 'npm_package_nameAlias':\n                        return 'my_app';\n                    case 'npm_package_version':\n                        return '0.0.1';\n                    }\n                });\n        }\n        // run the cli\n        if (local.global.utility2_rollup || module !== require.main) {\n            break;\n        }\n        local.assetsDict['/assets.example.js'] =\n            local.assetsDict['/assets.example.js'] ||\n            local.fs.readFileSync(__filename, 'utf8');\n        // bug-workaround - long $npm_package_buildCustomOrg\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.npmtest_kafka_node.rollup.js'] =\n            local.assetsDict['/assets.npmtest_kafka_node.rollup.js'] ||\n            local.fs.readFileSync(\n                local.npmtest_kafka_node.__dirname + '/lib.npmtest_kafka_node.js',\n                'utf8'\n            ).replace((/^#!/), '//');\n        /* jslint-ignore-end */\n        local.assetsDict['/favicon.ico'] = local.assetsDict['/favicon.ico'] || '';\n        // if $npm_config_timeout_exit exists,\n        // then exit this process after $npm_config_timeout_exit ms\n        if (Number(process.env.npm_config_timeout_exit)) {\n            setTimeout(process.exit, Number(process.env.npm_config_timeout_exit));\n        }\n        // start server\n        if (local.global.utility2_serverHttp1) {\n            break;\n        }\n        process.env.PORT = process.env.PORT || '8081';\n        console.error('server starting on port ' + process.env.PORT);\n        local.http.createServer(function (request, response) {\n            request.urlParsed = local.url.parse(request.url);\n            if (local.assetsDict[request.urlParsed.pathname] !== undefined) {\n                response.end(local.assetsDict[request.urlParsed.pathname]);\n                return;\n            }\n            response.statusCode = 404;\n            response.end();\n        }).listen(process.env.PORT);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/kafka.js":"exports.HighLevelConsumer = require('./lib/highLevelConsumer');\nexports.HighLevelProducer = require('./lib/highLevelProducer');\nexports.ConsumerGroup = require('./lib/consumerGroup');\nexports.Consumer = require('./lib/consumer');\nexports.Producer = require('./lib/producer');\nexports.Client = require('./lib/client');\nexports.Offset = require('./lib/offset');\nexports.KeyedMessage = require('./lib/protocol').KeyedMessage;\nexports.DefaultPartitioner = require('./lib/partitioner').DefaultPartitioner;\nexports.CyclicPartitioner = require('./lib/partitioner').CyclicPartitioner;\nexports.RandomPartitioner = require('./lib/partitioner').RandomPartitioner;\nexports.KeyedPartitioner = require('./lib/partitioner').KeyedPartitioner;\nexports.CustomPartitioner = require('./lib/partitioner').CustomPartitioner;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/highLevelConsumer.js":"'use strict';\n\nvar util = require('util');\nvar _ = require('lodash');\nvar events = require('events');\nvar uuid = require('uuid');\nvar async = require('async');\nvar errors = require('./errors');\nvar retry = require('retry');\nvar logger = require('./logging')('kafka-node:HighLevelConsumer');\nvar validateConfig = require('./utils').validateConfig;\n\nvar DEFAULTS = {\n  groupId: 'kafka-node-group',\n  // Auto commit config\n  autoCommit: true,\n  autoCommitIntervalMs: 5000,\n  // Fetch message config\n  fetchMaxWaitMs: 100,\n  paused: false,\n  maxNumSegments: 1000,\n  fetchMinBytes: 1,\n  fetchMaxBytes: 1024 * 1024,\n  maxTickMessages: 1000,\n  fromOffset: false,\n  rebalanceRetry: {\n    retries: 10,\n    factor: 2,\n    minTimeout: 1 * 100,\n    maxTimeout: 1 * 1000,\n    randomize: true\n  }\n};\n\nvar HighLevelConsumer = function (client, topics, options) {\n  if (!topics) {\n    throw new Error('Must have payloads');\n  }\n  this.fetchCount = 0;\n  this.client = client;\n  this.options = _.defaults((options || {}), DEFAULTS);\n  this.initialised = false;\n  this.ready = false;\n  this.closing = false;\n  this.paused = this.options.paused;\n  this.rebalancing = false;\n  this.pendingRebalances = 0;\n  this.committing = false;\n  this.needToCommit = false;\n  this.id = this.options.id || this.options.groupId + '_' + uuid.v4();\n  this.payloads = this.buildPayloads(topics);\n  this.topicPayloads = this.buildTopicPayloads(topics);\n  this.connect();\n\n  if (this.options.groupId) {\n    validateConfig('options.groupId', this.options.groupId);\n  }\n};\nutil.inherits(HighLevelConsumer, events.EventEmitter);\n\nHighLevelConsumer.prototype.buildPayloads = function (payloads) {\n  var self = this;\n  return payloads.map(function (p) {\n    if (typeof p !== 'object') p = { topic: p };\n    p.partition = p.partition || 0;\n    p.offset = p.offset || 0;\n    p.maxBytes = self.options.fetchMaxBytes;\n    p.metadata = 'm'; // metadata can be arbitrary\n    return p;\n  });\n};\n\n// Initially this will always be empty - until after a re-balance\nHighLevelConsumer.prototype.buildTopicPayloads = function (topics) {\n  return topics.map(function (j) {\n    var k = { topic: j.topic };\n    return k;\n  });\n};\n\n// Provide the topic payloads if requested\nHighLevelConsumer.prototype.getTopicPayloads = function () {\n  if (!this.rebalancing) return this.topicPayloads;\n  return null;\n};\n\nHighLevelConsumer.prototype.connect = function () {\n  var self = this;\n  // Client alreadyexists\n  if (this.client.ready) {\n    this.init();\n  }\n\n  this.client.on('ready', function () {\n    if (!self.initialised) self.init();\n\n    // Check the topics exist and create a watcher on them\n    var topics = self.payloads.map(function (p) {\n      return p.topic;\n    });\n\n    self.client.topicExists(topics, function (err) {\n      if (err) {\n        return self.emit('error', err);\n      }\n      self.initialised = true;\n    });\n  });\n\n  function checkPartitionOwnership (callback) {\n    async.each(self.topicPayloads, function (tp, cbb) {\n      if (tp.partition !== undefined) {\n        self.client.zk.checkPartitionOwnership(self.id, self.options.groupId, tp.topic, tp.partition, function (err) {\n          if (err) {\n            cbb(err);\n          } else {\n            cbb();\n          }\n        });\n      } else {\n        cbb();\n      }\n    }, callback);\n  }\n\n  // Check partition ownership and registration\n  this.checkPartitionOwnershipInterval = setInterval(function () {\n    if (!self.rebalancing) {\n      async.parallel([\n        checkPartitionOwnership,\n        function (callback) {\n          self.client.zk.isConsumerRegistered(self.options.groupId, self.id, function (error, registered) {\n            if (error) {\n              return callback(error);\n            }\n            if (registered) {\n              callback();\n            } else {\n              callback(new Error(util.format('Consumer %s is not registered in group %s', self.id, self.options.groupId)));\n            }\n          });\n        }\n      ], function (error) {\n        if (error) {\n          self.emit('error', new errors.FailedToRegisterConsumerError(error.toString(), error));\n        }\n      });\n    }\n  }, 20000);\n\n  function fetchAndUpdateOffsets (cb) {\n    self.fetchOffset(self.topicPayloads, function (err, topics) {\n      if (err) {\n        return cb(err);\n      }\n\n      self.ready = true;\n      self.updateOffsets(topics, true);\n\n      return cb();\n    });\n  }\n\n  function rebalance () {\n    logger.debug('rebalance() %s is rebalancing: %s ready: %s', self.id, self.rebalancing, self.ready);\n    if (!self.rebalancing && !self.closing) {\n      deregister();\n\n      self.emit('rebalancing');\n\n      self.rebalancing = true;\n      logger.debug('HighLevelConsumer rebalance retry config: %s', JSON.stringify(self.options.rebalanceRetry));\n      var oldTopicPayloads = self.topicPayloads;\n      var operation = retry.operation(self.options.rebalanceRetry);\n\n      operation.attempt(function (currentAttempt) {\n        self.rebalanceAttempt(oldTopicPayloads, function (err) {\n          if (operation.retry(err)) {\n            return;\n          }\n          if (err) {\n            self.rebalancing = false;\n            return self.emit('error', new errors.FailedToRebalanceConsumerError(operation.mainError().toString()));\n          } else {\n            var topicNames = self.topicPayloads.map(function (p) {\n              return p.topic;\n            });\n            self.client.refreshMetadata(topicNames, function (err) {\n              register();\n              if (err) {\n                self.rebalancing = false;\n                self.emit('error', err);\n                return;\n              }\n\n              if (self.topicPayloads.length) {\n                fetchAndUpdateOffsets(function (err) {\n                  self.rebalancing = false;\n                  if (err) {\n                    self.emit('error', new errors.FailedToRebalanceConsumerError(err.message));\n                    return;\n                  }\n                  self.fetch();\n                  self.emit('rebalanced');\n                });\n              } else { // was not assigned any partitions during rebalance\n                self.rebalancing = false;\n                self.emit('rebalanced');\n              }\n            });\n          }\n        });\n      });\n    }\n  }\n\n  // Wait for the consumer to be ready\n  this.on('registered', rebalance);\n\n  function register (fn) {\n    logger.debug('Registered listeners %s', self.id);\n    self.client.zk.on('consumersChanged', fn || rebalance);\n    self.client.zk.on('partitionsChanged', fn || rebalance);\n    self.client.on('brokersChanged', fn || rebalance);\n  }\n\n  function deregister (fn) {\n    logger.debug('Deregistered listeners %s', self.id);\n    self.client.zk.removeListener('consumersChanged', fn || rebalance);\n    self.client.zk.removeListener('partitionsChanged', fn || rebalance);\n    self.client.removeListener('brokersChanged', fn || rebalance);\n  }\n\n  function pendingRebalance () {\n    if (self.rebalancing) {\n      self.pendingRebalances++;\n      logger.debug('%s added a pendingRebalances %d', self.id, self.pendingRebalances);\n    }\n  }\n\n  function attachZookeeperErrorListener () {\n    self.client.zk.on('error', function (err) {\n      self.emit('error', err);\n    });\n  }\n\n  attachZookeeperErrorListener();\n\n  this.client.on('zkReconnect', function () {\n    logger.debug('zookeeper reconnect for %s', self.id);\n    attachZookeeperErrorListener();\n\n    // clean up what's leftover\n    self.leaveGroup(function () {\n      // rejoin the group\n      self.registerConsumer(function (error) {\n        if (error) {\n          return self.emit('error', new errors.FailedToRegisterConsumerError('Failed to register consumer on zkReconnect', error));\n        }\n        self.emit('registered');\n      });\n    });\n  });\n\n  this.on('rebalanced', function () {\n    deregister(pendingRebalance);\n    if (self.pendingRebalances && !self.closing) {\n      logger.debug('%s pendingRebalances is %d scheduling a rebalance...', self.id, self.pendingRebalances);\n      setTimeout(function () {\n        logger.debug('%s running scheduled rebalance', self.id);\n        rebalance();\n      }, 500);\n    }\n  });\n\n  this.on('rebalancing', function () {\n    register(pendingRebalance);\n    self.pendingRebalances = 0;\n  });\n\n  this.client.on('error', function (err) {\n    self.emit('error', err);\n  });\n\n  this.client.on('reconnect', function (lastError) {\n    self.fetch();\n  });\n\n  this.client.on('close', function () {\n    logger.debug('close');\n  });\n\n  this.on('offsetOutOfRange', function (topic) {\n    self.pause();\n    topic.maxNum = self.options.maxNumSegments;\n    topic.metadata = 'm';\n    topic.time = Date.now();\n    self.offsetRequest([topic], function (err, offsets) {\n      if (err) {\n        self.emit('error', new errors.InvalidConsumerOffsetError(self));\n      } else {\n        var min = Math.min.apply(null, offsets[topic.topic][topic.partition]);\n        // set minimal offset\n        self.setOffset(topic.topic, topic.partition, min);\n        self.resume();\n      }\n    });\n  });\n\n  // 'done' will be emit when a message fetch request complete\n  this.on('done', function (topics) {\n    self.updateOffsets(topics);\n    if (!self.paused) {\n      setImmediate(function () {\n        self.fetch();\n      });\n    }\n  });\n};\n\nHighLevelConsumer.prototype._releasePartitions = function (topicPayloads, callback) {\n  var self = this;\n  async.each(topicPayloads, function (tp, cbb) {\n    if (tp.partition !== undefined) {\n      async.series([\n        function (delcbb) {\n          self.client.zk.checkPartitionOwnership(self.id, self.options.groupId, tp.topic, tp.partition, function (err) {\n            if (err) {\n              // Partition doesn't exist simply carry on\n              cbb();\n            } else delcbb();\n          });\n        },\n        function (delcbb) {\n          self.client.zk.deletePartitionOwnership(self.options.groupId, tp.topic, tp.partition, delcbb);\n        },\n        function (delcbb) {\n          self.client.zk.checkPartitionOwnership(self.id, self.options.groupId, tp.topic, tp.partition, function (err) {\n            if (err) {\n              delcbb();\n            } else {\n              delcbb('Partition should not exist');\n            }\n          });\n        }],\n      cbb);\n    } else {\n      cbb();\n    }\n  }, callback);\n};\n\nHighLevelConsumer.prototype.rebalanceAttempt = function (oldTopicPayloads, cb) {\n  var self = this;\n  // Do the rebalance.....\n  var consumerPerTopicMap;\n  var newTopicPayloads = [];\n  logger.debug('HighLevelConsumer %s is attempting to rebalance', self.id);\n  async.series([\n\n    // Stop fetching data and commit offsets\n    function (callback) {\n      logger.debug('HighLevelConsumer %s stopping data read during rebalance', self.id);\n      self.stop(function () {\n        callback();\n      });\n    },\n\n    // Assemble the data\n    function (callback) {\n      logger.debug('HighLevelConsumer %s assembling data for rebalance', self.id);\n      self.client.zk.getConsumersPerTopic(self.options.groupId, function (err, obj) {\n        if (err) {\n          callback(err);\n        } else {\n          consumerPerTopicMap = obj;\n          callback();\n        }\n      });\n    },\n\n    // Release current partitions\n    function (callback) {\n      logger.debug('HighLevelConsumer %s releasing current partitions during rebalance', self.id);\n      self._releasePartitions(oldTopicPayloads, callback);\n    },\n\n    // Rebalance\n    function (callback) {\n      logger.debug('HighLevelConsumer %s determining the partitions to own during rebalance', self.id);\n      logger.debug('consumerPerTopicMap.consumerTopicMap %j', consumerPerTopicMap.consumerTopicMap);\n      for (var topic in consumerPerTopicMap.consumerTopicMap[self.id]) {\n        if (!consumerPerTopicMap.consumerTopicMap[self.id].hasOwnProperty(topic)) {\n          continue;\n        }\n        var topicToAdd = consumerPerTopicMap.consumerTopicMap[self.id][topic];\n        var numberOfConsumers = consumerPerTopicMap.topicConsumerMap[topicToAdd].length;\n        var numberOfPartition = consumerPerTopicMap.topicPartitionMap[topicToAdd].length;\n        var partitionsPerConsumer = Math.floor(numberOfPartition / numberOfConsumers);\n        var extraPartitions = numberOfPartition % numberOfConsumers;\n        var currentConsumerIndex;\n        for (var index in consumerPerTopicMap.topicConsumerMap[topicToAdd]) {\n          if (!consumerPerTopicMap.topicConsumerMap[topicToAdd].hasOwnProperty(index)) {\n            continue;\n          }\n          if (consumerPerTopicMap.topicConsumerMap[topicToAdd][index] === self.id) {\n            currentConsumerIndex = parseInt(index);\n            break;\n          }\n        }\n        var extraBit = currentConsumerIndex;\n        if (currentConsumerIndex > extraPartitions) extraBit = extraPartitions;\n        var startPart = partitionsPerConsumer * currentConsumerIndex + extraBit;\n        var extraNParts = 1;\n        if (currentConsumerIndex + 1 > extraPartitions) extraNParts = 0;\n        var nParts = partitionsPerConsumer + extraNParts;\n\n        for (var i = startPart; i < startPart + nParts; i++) {\n          newTopicPayloads.push({\n            topic: topicToAdd,\n            partition: consumerPerTopicMap.topicPartitionMap[topicToAdd][i],\n            offset: 0,\n            maxBytes: self.options.fetchMaxBytes,\n            metadata: 'm'\n          });\n        }\n      }\n      logger.debug('newTopicPayloads %j', newTopicPayloads);\n      callback();\n    },\n\n    // Update ZK with new ownership\n    function (callback) {\n      if (newTopicPayloads.length) {\n        logger.debug('HighLevelConsumer %s gaining ownership of partitions during rebalance', self.id);\n        async.eachSeries(newTopicPayloads, function (tp, cbb) {\n          if (tp.partition !== undefined) {\n            async.series([\n              function (addcbb) {\n                self.client.zk.checkPartitionOwnership(self.id, self.options.groupId, tp.topic, tp.partition, function (err) {\n                  if (err) {\n                    // Partition doesn't exist simply carry on\n                    addcbb();\n                  } else cbb(); // Partition exists simply carry on\n                });\n              },\n              function (addcbb) {\n                self.client.zk.addPartitionOwnership(self.id, self.options.groupId, tp.topic, tp.partition, function (err) {\n                  if (err) {\n                    addcbb(err);\n                  } else addcbb();\n                });\n              }],\n              function (err) {\n                if (err) {\n                  cbb(err);\n                } else cbb();\n              });\n          } else {\n            cbb();\n          }\n        }, function (err) {\n          if (err) {\n            callback(err);\n          } else {\n            callback();\n          }\n        });\n      } else {\n        logger.debug('HighLevelConsumer %s has been assigned no partitions during rebalance', self.id);\n        callback();\n      }\n    },\n\n    // Update the new topic offsets\n    function (callback) {\n      self.topicPayloads = newTopicPayloads;\n      callback();\n    }],\n    function (err) {\n      if (err) {\n        logger.debug('HighLevelConsumer %s rebalance attempt failed', self.id);\n        cb(err);\n      } else {\n        logger.debug('HighLevelConsumer %s rebalance attempt was successful', self.id);\n        cb();\n      }\n    });\n};\n\nHighLevelConsumer.prototype.init = function () {\n  var self = this;\n\n  if (!self.topicPayloads.length) {\n    return;\n  }\n\n  self.registerConsumer(function (err) {\n    if (err) {\n      return self.emit('error', new errors.FailedToRegisterConsumerError(err.toString()));\n    }\n\n    // Close the\n    return self.emit('registered');\n  });\n};\n\n/*\n * Update offset info in current payloads\n * @param {Object} Topic-partition-offset\n * @param {Boolean} Don't commit when initing consumer\n */\nHighLevelConsumer.prototype.updateOffsets = function (topics, initing) {\n  this.topicPayloads.forEach(p => {\n    if (!_.isEmpty(topics[p.topic]) && topics[p.topic][p.partition] !== undefined) {\n      var offset = topics[p.topic][p.partition];\n      if (offset === -1) offset = 0;\n      if (!initing) p.offset = offset + 1;\n      else p.offset = offset;\n      this.needToCommit = true;\n    }\n  });\n\n  if (this.options.autoCommit && !initing) {\n    this.autoCommit(false, function (err) {\n      err && logger.debug('auto commit offset', err);\n    });\n  }\n};\n\nHighLevelConsumer.prototype.sendOffsetCommitRequest = function (commits, cb) {\n  this.client.sendOffsetCommitRequest(this.options.groupId, commits, cb);\n};\n\nfunction autoCommit (force, cb) {\n  if (arguments.length === 1) {\n    cb = force;\n    force = false;\n  }\n\n  if (!force) {\n    if (this.committing) return cb(null, 'Offset committing');\n    if (!this.needToCommit) return cb(null, 'Commit not needed');\n  }\n\n  this.needToCommit = false;\n  this.committing = true;\n  setTimeout(function () {\n    this.committing = false;\n  }.bind(this), this.options.autoCommitIntervalMs);\n\n  var commits = this.topicPayloads.filter(function (p) { return p.offset !== -1; });\n\n  if (commits.length) {\n    this.sendOffsetCommitRequest(commits, cb);\n  } else {\n    cb(null, 'Nothing to be committed');\n  }\n}\nHighLevelConsumer.prototype.commit = HighLevelConsumer.prototype.autoCommit = autoCommit;\n\nHighLevelConsumer.prototype.fetch = function () {\n  if (!this.ready || this.rebalancing || this.paused) {\n    return;\n  }\n\n  this.client.sendFetchRequest(this, this.topicPayloads, this.options.fetchMaxWaitMs, this.options.fetchMinBytes, this.options.maxTickMessages);\n};\n\nHighLevelConsumer.prototype.fetchOffset = function (payloads, cb) {\n  logger.debug('in fetchOffset %s payloads: %j', this.id, payloads);\n  this.client.sendOffsetFetchRequest(this.options.groupId, payloads, cb);\n};\n\nHighLevelConsumer.prototype.offsetRequest = function (payloads, cb) {\n  this.client.sendOffsetRequest(payloads, cb);\n};\n\n/**\n * Register a consumer against a group\n *\n * @param consumer to register\n *\n * @param {Client~failedToRegisterConsumerCallback} cb A function to call the consumer has been registered\n */\nHighLevelConsumer.prototype.registerConsumer = function (cb) {\n  var self = this;\n  var groupId = this.options.groupId;\n  this.client.zk.registerConsumer(groupId, this.id, this.payloads, function (err) {\n    if (err) return cb(err);\n    self.client.zk.listConsumers(self.options.groupId);\n    var topics = self.topicPayloads.reduce(function (ret, topicPayload) {\n      if (ret.indexOf(topicPayload.topic) === -1) {\n        ret.push(topicPayload.topic);\n      }\n      return ret;\n    }, []);\n    topics.forEach(function (topic) {\n      self.client.zk.listPartitions(topic);\n    });\n    cb();\n  });\n};\n\nHighLevelConsumer.prototype.addTopics = function (topics, cb) {\n  var self = this;\n  if (!this.ready) {\n    setTimeout(function () {\n      self.addTopics(topics, cb);\n    }, 100);\n    return;\n  }\n  this.client.addTopics(\n    topics,\n    function (err, added) {\n      if (err) return cb && cb(err, added);\n\n      var payloads = self.buildPayloads(topics);\n      // update offset of topics that will be added\n      self.fetchOffset(payloads, function (err, offsets) {\n        if (err) return cb(err);\n        payloads.forEach(function (p) {\n          var offset = offsets[p.topic][p.partition];\n          if (offset === -1) offset = 0;\n          p.offset = offset;\n          self.topicPayloads.push(p);\n        });\n        // TODO: rebalance consumer\n        cb && cb(null, added);\n      });\n    }\n  );\n};\n\nHighLevelConsumer.prototype.removeTopics = function (topics, cb) {\n  topics = typeof topics === 'string' ? [topics] : topics;\n  this.payloads = this.payloads.filter(function (p) {\n    return !~topics.indexOf(p.topic);\n  });\n\n  this.client.removeTopicMetadata(topics, cb);\n};\n\nHighLevelConsumer.prototype.leaveGroup = function (cb) {\n  var self = this;\n  async.parallel([\n    function (callback) {\n      if (self.topicPayloads.length) {\n        self._releasePartitions(self.topicPayloads, callback);\n      } else {\n        callback(null);\n      }\n    },\n    function (callback) {\n      self.client.zk.unregisterConsumer(self.options.groupId, self.id, callback);\n    }\n  ], cb);\n};\n\nHighLevelConsumer.prototype.close = function (force, cb) {\n  var self = this;\n  this.ready = false;\n  this.closing = true;\n  clearInterval(this.checkPartitionOwnershipInterval);\n\n  if (typeof force === 'function') {\n    cb = force;\n    force = false;\n  }\n\n  async.series([\n    function (callback) {\n      self.leaveGroup(callback);\n    },\n    function (callback) {\n      if (force) {\n        async.series([\n          function (callback) {\n            self.commit(true, callback);\n          },\n          function (callback) {\n            self.client.close(callback);\n          }\n        ], callback);\n        return;\n      }\n      self.client.close(callback);\n    }\n  ], cb);\n};\n\nHighLevelConsumer.prototype.stop = function (cb) {\n  if (!this.options.autoCommit) return cb && cb();\n  this.commit(true, function (err) {\n    cb && cb(err);\n  });\n};\n\nHighLevelConsumer.prototype.setOffset = function (topic, partition, offset) {\n  this.topicPayloads.every(function (p) {\n    if (p.topic === topic && p.partition == partition) { // eslint-disable-line eqeqeq\n      p.offset = offset;\n      return false;\n    }\n    return true;\n  });\n};\n\nHighLevelConsumer.prototype.pause = function () {\n  this.paused = true;\n};\n\nHighLevelConsumer.prototype.resume = function () {\n  this.paused = false;\n  this.fetch();\n};\n\nmodule.exports = HighLevelConsumer;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/index.js":"module.exports = {\n  BrokerNotAvailableError: require('./BrokerNotAvailableError'),\n  TopicsNotExistError: require('./TopicsNotExistError'),\n  FailedToRegisterConsumerError: require('./FailedToRegisterConsumerError'),\n  InvalidConsumerOffsetError: require('./InvalidConsumerOffsetError'),\n  FailedToRebalanceConsumerError: require('./FailedToRebalanceConsumerError'),\n  InvalidConfigError: require('./InvalidConfigError'),\n  ConsumerGroupErrors: [\n    require('./GroupCoordinatorNotAvailableError'),\n    require('./GroupLoadInProgressError'),\n    require('./HeartbeatTimeoutError'),\n    require('./IllegalGenerationError'),\n    require('./NotCoordinatorForGroupError'),\n    require('./RebalanceInProgressError'),\n    require('./UnknownMemberIdError')\n  ]\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/BrokerNotAvailableError.js":"var util = require('util');\n\n/**\n * A broker/leader was not available or discoverable for the action requested\n *\n * @param {String} message A message describing the issue with the broker\n *\n * @constructor\n */\nvar BrokerNotAvailableError = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(BrokerNotAvailableError, Error);\nBrokerNotAvailableError.prototype.name = 'BrokerNotAvailableError';\n\nmodule.exports = BrokerNotAvailableError;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/TopicsNotExistError.js":"var util = require('util');\n\n/**\n * One or more topics did not exist for the requested action\n *\n * @param {String|String[]} topics Either an array or single topic name\n *\n * @constructor\n */\nvar TopicsNotExistError = function (topics) {\n  Error.captureStackTrace(this, this);\n  this.topics = topics;\n  this.message = 'The topic(s) ' + topics.toString() + ' do not exist';\n};\n\nutil.inherits(TopicsNotExistError, Error);\nTopicsNotExistError.prototype.name = 'TopicsNotExistError';\n\nmodule.exports = TopicsNotExistError;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/FailedToRegisterConsumerError.js":"var util = require('util');\nvar NestedError = require('nested-error-stacks');\n\n/**\n * Failed to register the consumer\n *\n * @param {String} message A message describing the problem with the registration of the consumer\n * @param {Error} error An error related to the registration of the consumer\n *\n * @constructor\n */\nvar FailedToRegisterConsumerError = function (message, nested) {\n  NestedError.call(this, message, nested);\n  this.message = message;\n};\n\nutil.inherits(FailedToRegisterConsumerError, NestedError);\nFailedToRegisterConsumerError.prototype.name = 'FailedToRegisterConsumerError';\n\nmodule.exports = FailedToRegisterConsumerError;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/InvalidConsumerOffsetError.js":"'use strict';\n\nconst util = require('util');\nconst NestedError = require('nested-error-stacks');\n\n/**\n * The offset for the comsumer is invalid\n *\n * @param {String} message A message describing the problem with the fetching of offsets for the consumer\n *\n * @constructor\n */\nconst InvalidConsumerOffsetError = function (message, nested) {\n  NestedError.apply(this, arguments);\n};\n\nutil.inherits(InvalidConsumerOffsetError, NestedError);\nInvalidConsumerOffsetError.prototype.name = 'InvalidConsumerOffsetError';\n\nmodule.exports = InvalidConsumerOffsetError;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/FailedToRebalanceConsumerError.js":"var util = require('util');\n\n/**\n * Failed to rebalance the consumer\n *\n * @param {String} message A message describing the error during rebalancing of the consumer\n *\n * @constructor\n */\nvar FailedToRebalanceConsumerError = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(FailedToRebalanceConsumerError, Error);\nFailedToRebalanceConsumerError.prototype.name = 'FailedToRebalanceConsumerError';\n\nmodule.exports = FailedToRebalanceConsumerError;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/InvalidConfigError.js":"var util = require('util');\n\nvar InvalidConfigError = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(InvalidConfigError, Error);\nInvalidConfigError.prototype.name = 'InvalidConfigError';\n\nmodule.exports = InvalidConfigError;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/GroupCoordinatorNotAvailableError.js":"var util = require('util');\n\nvar GroupCoordinatorNotAvailable = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(GroupCoordinatorNotAvailable, Error);\nGroupCoordinatorNotAvailable.prototype.name = 'GroupCoordinatorNotAvailable';\n\nmodule.exports = GroupCoordinatorNotAvailable;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/GroupLoadInProgressError.js":"var util = require('util');\n\nvar GroupLoadInProgress = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(GroupLoadInProgress, Error);\nGroupLoadInProgress.prototype.name = 'GroupLoadInProgress';\n\nmodule.exports = GroupLoadInProgress;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/HeartbeatTimeoutError.js":"'use strict';\n\nvar util = require('util');\n\nvar HeartbeatTimeout = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(HeartbeatTimeout, Error);\nHeartbeatTimeout.prototype.name = 'HeartbeatTimeout';\n\nmodule.exports = HeartbeatTimeout;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/IllegalGenerationError.js":"var util = require('util');\n\nvar IllegalGeneration = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(IllegalGeneration, Error);\nIllegalGeneration.prototype.name = 'IllegalGeneration';\n\nmodule.exports = IllegalGeneration;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/NotCoordinatorForGroupError.js":"var util = require('util');\n\nvar NotCoordinatorForGroup = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(NotCoordinatorForGroup, Error);\nNotCoordinatorForGroup.prototype.name = 'NotCoordinatorForGroup';\n\nmodule.exports = NotCoordinatorForGroup;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/RebalanceInProgressError.js":"var util = require('util');\n\nvar RebalanceInProgress = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(RebalanceInProgress, Error);\nRebalanceInProgress.prototype.name = 'RebalanceInProgress';\n\nmodule.exports = RebalanceInProgress;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/errors/UnknownMemberIdError.js":"var util = require('util');\n\nvar UnknownMemberId = function (message) {\n  Error.captureStackTrace(this, this);\n  this.message = message;\n};\n\nutil.inherits(UnknownMemberId, Error);\nUnknownMemberId.prototype.name = 'UnknownMemberId';\n\nmodule.exports = UnknownMemberId;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/logging.js":"'use strict';\n\nconst debug = require('debug');\n\nlet loggerProvider = debugLoggerProvider;\n\nmodule.exports = exports = function getLogger (name) {\n  return loggerProvider(name);\n};\n\nexports.setLoggerProvider = function setLoggerProvider (provider) {\n  loggerProvider = provider;\n};\n\nfunction debugLoggerProvider (name) {\n  let logger = debug(name);\n  logger = logger.bind(logger);\n\n  return {\n    debug: logger,\n    info: logger,\n    warn: logger,\n    error: logger\n  };\n}\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/utils.js":"var assert = require('assert');\nvar InvalidConfigError = require('./errors/InvalidConfigError');\nvar legalChars = new RegExp('^[a-zA-Z0-9._-]*$');\nconst allowedTopicLength = 249;\n\nfunction validateConfig (property, value) {\n  if (!legalChars.test(value)) {\n    throw new InvalidConfigError([property, value, \"is illegal, contains a character other than ASCII alphanumerics, '.', '_' and '-'\"].join(' '));\n  }\n}\n\nfunction validateTopicNames (topics) {\n  // Rewriting same validations done by Apache Kafka team for topics\n  // iterating over topics\n  topics.some(function (topic) {\n    if (topic.length <= 0) {\n      throw new InvalidConfigError('topic name is illegal, cannot be empty');\n    }\n    if (topic === '.' || topic === '..') {\n      throw new InvalidConfigError('topic name cannot be . or ..');\n    }\n    if (topic.length > allowedTopicLength) {\n      throw new InvalidConfigError(`topic name is illegal, cannot be longer than ${allowedTopicLength} characters`);\n    }\n    if (!legalChars.test(topic)) {\n      throw new InvalidConfigError(`topic name ${topic} is illegal, contains a character other than ASCII alphanumerics .,_ and -`);\n    }\n  });\n  return true;\n}\n\nfunction validateTopics (topics) {\n  if (topics.some(function (topic) {\n    if ('partition' in topic) {\n      return typeof topic.partition !== 'number';\n    }\n    return false;\n  })) {\n    throw new InvalidConfigError('Offset must be a number and can not contain characters');\n  }\n}\n\n/*\nConverts:\n\n  [\n    {topic: 'test', partition: 0},\n    {topic: 'test', partition: 1},\n    {topic: 'Bob', partition: 0}\n  ]\n\nInto:\n\n  {\n    test: [0, 1],\n    bob: [0]\n  }\n\n*/\nfunction groupPartitionsByTopic (topicPartitions) {\n  assert(Array.isArray(topicPartitions));\n  return topicPartitions.reduce(function (result, tp) {\n    if (!(tp.topic in result)) {\n      result[tp.topic] = [tp.partition];\n    } else {\n      result[tp.topic].push(tp.partition);\n    }\n    return result;\n  }, {});\n}\n\n/*\nConverts:\n  {\n    test: [0, 1],\n    bob: [0]\n  }\n\nInto a topic partition payload:\n  [\n    {topic: 'test', partition: 0},\n    {topic: 'test', partition: 1},\n    {topic: 'Bob', partition: 0}\n  ]\n*/\nfunction createTopicPartitionList (topicPartitions) {\n  var tpList = [];\n  for (var topic in topicPartitions) {\n    if (!topicPartitions.hasOwnProperty(topic)) {\n      continue;\n    }\n    topicPartitions[topic].forEach(function (partition) {\n      tpList.push({\n        topic: topic,\n        partition: partition\n      });\n    });\n  }\n  return tpList;\n}\n\nmodule.exports = {\n  validateConfig: validateConfig,\n  validateTopics: validateTopics,\n  groupPartitionsByTopic: groupPartitionsByTopic,\n  createTopicPartitionList: createTopicPartitionList,\n  validateTopicNames: validateTopicNames\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/highLevelProducer.js":"'use strict';\n\nvar util = require('util');\nvar BaseProducer = require('./baseProducer');\n\n/** @inheritdoc */\nfunction HighLevelProducer (client, options, customPartitioner) {\n  BaseProducer.call(this, client, options, BaseProducer.PARTITIONER_TYPES.cyclic, customPartitioner);\n}\n\nutil.inherits(HighLevelProducer, BaseProducer);\n\nHighLevelProducer.PARTITIONER_TYPES = BaseProducer.PARTITIONER_TYPES;\n\nmodule.exports = HighLevelProducer;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/baseProducer.js":"'use strict';\n\nvar assert = require('assert');\nvar util = require('util');\nvar events = require('events');\nvar _ = require('lodash');\nvar protocol = require('./protocol');\nvar Message = protocol.Message;\nvar KeyedMessage = protocol.KeyedMessage;\nvar ProduceRequest = protocol.ProduceRequest;\nvar partitioner = require('./partitioner');\nvar DefaultPartitioner = partitioner.DefaultPartitioner;\nvar RandomPartitioner = partitioner.RandomPartitioner;\nvar CyclicPartitioner = partitioner.CyclicPartitioner;\nvar KeyedPartitioner = partitioner.KeyedPartitioner;\nvar CustomPartitioner = partitioner.CustomPartitioner;\n\nvar PARTITIONER_TYPES = {\n  default: 0,\n  random: 1,\n  cyclic: 2,\n  keyed: 3,\n  custom: 4\n};\n\nvar PARTITIONER_MAP = {\n  0: DefaultPartitioner,\n  1: RandomPartitioner,\n  2: CyclicPartitioner,\n  3: KeyedPartitioner,\n  4: CustomPartitioner\n};\n\nvar DEFAULTS = {\n  requireAcks: 1,\n  ackTimeoutMs: 100\n};\n\n/**\n * Provides common functionality for a kafka producer\n *\n * @param {Client} client A kafka client object to use for the producer\n * @param {Object} [options] An object containing configuration options\n * @param {Number} [options.requireAcks=1] Configuration for when to consider a message as acknowledged.\n *      <li>0 = No ack required</li>\n *      <li>1 = Leader ack required</li>\n *      <li>-1 = All in sync replicas ack required</li>\n *\n * @param {Number} [options.ackTimeoutMs=100] The amount of time in milliseconds to wait for all acks before considered\n *      the message as errored\n * @param {Number} [defaultPartitionType] The default partitioner type\n * @param {Object} [customPartitioner] a custom partitinoer to use of the form: function (partitions, key)\n * @constructor\n */\nfunction BaseProducer (client, options, defaultPartitionerType, customPartitioner) {\n  options = options || {};\n\n  this.ready = false;\n  this.client = client;\n\n  this.requireAcks = options.requireAcks === undefined\n    ? DEFAULTS.requireAcks\n    : options.requireAcks;\n  this.ackTimeoutMs = options.ackTimeoutMs === undefined\n    ? DEFAULTS.ackTimeoutMs\n    : options.ackTimeoutMs;\n\n  if (customPartitioner !== undefined && options.partitionerType !== PARTITIONER_TYPES.custom) {\n    throw new Error('Partitioner Type must be custom if providing a customPartitioner.');\n  } else if (customPartitioner === undefined && options.partitionerType === PARTITIONER_TYPES.custom) {\n    throw new Error('No customer partitioner defined');\n  }\n\n  var partitionerType = PARTITIONER_MAP[options.partitionerType] || PARTITIONER_MAP[defaultPartitionerType];\n\n  // eslint-disable-next-line\n  this.partitioner = new partitionerType(customPartitioner);\n\n  this.connect();\n}\n\nutil.inherits(BaseProducer, events.EventEmitter);\n\nBaseProducer.prototype.connect = function () {\n  // emiter...\n  var self = this;\n  this.ready = this.client.ready;\n  if (this.ready) self.emit('ready');\n  this.client.on('ready', function () {\n    if (!self.ready) {\n      self.ready = true;\n      self.emit('ready');\n    }\n  });\n  this.client.on('brokersChanged', function () {\n    let topics = Object.keys(this.topicMetadata);\n    this.refreshMetadata(topics, function (error) {\n      if (error) {\n        self.emit('error', error);\n      }\n    });\n  });\n  this.client.on('error', function (err) {\n    self.emit('error', err);\n  });\n  this.client.on('close', function () {});\n};\n\n/**\n * Sends a new message or array of messages to a topic/partition\n * This will use the\n *\n * @see Client#sendProduceRequest for a more low level way to send messages to kafka\n *\n * @param {Array.<BaseProducer~sendPayload>} payloads An array of topic payloads\n * @param {BaseProducer~sendCallback} cb A function to call once the send has completed\n */\nBaseProducer.prototype.send = function (payloads, cb) {\n  var client = this.client;\n  var requireAcks = this.requireAcks;\n  var ackTimeoutMs = this.ackTimeoutMs;\n\n  client.sendProduceRequest(this.buildPayloads(payloads, client.topicMetadata), requireAcks, ackTimeoutMs, cb);\n};\n\nBaseProducer.prototype.buildPayloads = function (payloads, topicMetadata) {\n  const topicPartitionRequests = Object.create(null);\n  payloads.forEach((p) => {\n    p.partition = p.hasOwnProperty('partition') ? p.partition : this.partitioner.getPartition(_.map(topicMetadata[p.topic], 'partition'), p.key);\n    p.attributes = p.hasOwnProperty('attributes') ? p.attributes : 0;\n    let messages = _.isArray(p.messages) ? p.messages : [p.messages];\n\n    messages = messages.map(function (message) {\n      if (message instanceof KeyedMessage) {\n        return message;\n      }\n      return new Message(0, 0, '', message);\n    });\n\n    let key = p.topic + p.partition;\n    let request = topicPartitionRequests[key];\n\n    if (request == null) {\n      topicPartitionRequests[key] = new ProduceRequest(p.topic, p.partition, messages, p.attributes);\n    } else {\n      assert(request.attributes === p.attributes);\n      Array.prototype.push.apply(request.messages, messages);\n    }\n  });\n  return _.values(topicPartitionRequests);\n};\n\nBaseProducer.prototype.createTopics = function (topics, async, cb) {\n  if (!this.ready) {\n    return cb(new Error('Producer not ready!'));\n  }\n\n  this.client.createTopics(topics, async, cb);\n};\n\nBaseProducer.prototype.close = function (cb) {\n  this.client.close(cb);\n};\n\nBaseProducer.PARTITIONER_TYPES = PARTITIONER_TYPES;\n\nmodule.exports = BaseProducer;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/protocol/index.js":"'use strict';\n\nvar _ = require('lodash');\nvar struct = require('./protocol_struct');\nvar protocol = require('./protocol');\n\nexports = _.extend(exports, struct, protocol);\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/protocol/protocol_struct.js":"'use strict';\n\nfunction createStruct () {\n  var args = arguments[0];\n  return function () {\n    for (var i = 0; i < args.length; i++) {\n      this[args[i]] = arguments[i];\n    }\n  };\n}\n\nvar KEYS = {\n  FetchRequest: ['topic', 'partition', 'offset', 'maxBytes'],\n  FetchResponse: ['topic', 'fetchPartitions'],\n  OffsetCommitRequest: ['topic', 'partition', 'offset', 'metadata', 'committing', 'autoCommitIntervalMs'],\n  OffsetCommitResponse: [],\n  TopicAndPartition: ['topic', 'partition'],\n  PartitionMetadata: ['topic', 'partition', 'leader', 'replicas', 'isr'],\n  Message: ['magic', 'attributes', 'key', 'value'],\n  ProduceRequest: ['topic', 'partition', 'messages', 'attributes'],\n  Request: ['payloads', 'encoder', 'decoder', 'callback']\n};\n\nvar ERROR_CODE = {\n  '0': 'NoError',\n  '-1': 'Unknown',\n  '1': 'OffsetOutOfRange',\n  '2': 'InvalidMessage',\n  '3': 'UnknownTopicOrPartition',\n  '4': 'InvalidMessageSize',\n  '5': 'LeaderNotAvailable',\n  '6': 'NotLeaderForPartition',\n  '7': 'RequestTimedOut',\n  '8': 'BrokerNotAvailable',\n  '9': 'ReplicaNotAvailable',\n  '10': 'MessageSizeTooLarge',\n  '11': 'StaleControllerEpochCode',\n  '12': 'OffsetMetadataTooLargeCode',\n  '14': 'GroupLoadInProgress',\n  '15': 'GroupCoordinatorNotAvailable',\n  '16': 'NotCoordinatorForGroup',\n  '22': 'IllegalGeneration',\n  '23': 'InconsistentGroupProtocol',\n  '25': 'UnknownMemberId',\n  '26': 'InvalidSessionTimeout',\n  '27': 'RebalanceInProgress',\n  '30': 'GroupAuthorizationFailed'\n};\n\nvar GROUP_ERROR = {\n  'GroupCoordinatorNotAvailable': require('../errors/GroupCoordinatorNotAvailableError'),\n  'IllegalGeneration': require('../errors/IllegalGenerationError'),\n  'NotCoordinatorForGroup': require('../errors/NotCoordinatorForGroupError'),\n  'GroupLoadInProgress': require('../errors/GroupLoadInProgressError'),\n  'UnknownMemberId': require('../errors/UnknownMemberIdError'),\n  'RebalanceInProgress': require('../errors/RebalanceInProgressError')\n};\n\nvar REQUEST_TYPE = {\n  produce: 0,\n  fetch: 1,\n  offset: 2,\n  metadata: 3,\n  leader: 4,\n  stopReplica: 5,\n  offsetCommit: 8,\n  offsetFetch: 9,\n  groupCoordinator: 10,\n  joinGroup: 11,\n  heartbeat: 12,\n  leaveGroup: 13,\n  syncGroup: 14\n};\n\nObject.keys(KEYS).forEach(function (o) { exports[o] = createStruct(KEYS[o]); });\nexports.KEYS = KEYS;\nexports.ERROR_CODE = ERROR_CODE;\nexports.GROUP_ERROR = GROUP_ERROR;\nexports.REQUEST_TYPE = REQUEST_TYPE;\nexports.KeyedMessage = function KeyedMessage (key, value) {\n  exports.Message.call(this, 0, 0, key, value);\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/protocol/protocol.js":"'use strict';\n\nvar Binary = require('binary');\nvar Buffermaker = require('buffermaker');\nvar _ = require('lodash');\nvar crc32 = require('buffer-crc32');\nvar protocol = require('./protocol_struct');\nvar getCodec = require('../codec');\nvar REQUEST_TYPE = protocol.REQUEST_TYPE;\nvar ERROR_CODE = protocol.ERROR_CODE;\nvar GROUP_ERROR = protocol.GROUP_ERROR;\nvar PartitionMetadata = protocol.PartitionMetadata;\n\nvar API_VERSION = 0;\nvar REPLICA_ID = -1;\nvar GROUPS_PROTOCOL_TYPE = 'consumer';\n\nfunction groupByTopic (payloads) {\n  return payloads.reduce(function (out, p) {\n    out[p.topic] = out[p.topic] || {};\n    out[p.topic][p.partition] = p;\n    return out;\n  }, {});\n}\n\nfunction encodeRequestWithLength (request) {\n  return new Buffermaker()\n    .Int32BE(request.length)\n    .string(request)\n    .make();\n}\n\nfunction encodeRequestHeader (clientId, correlationId, apiKey, apiVersion) {\n  return new Buffermaker()\n    .Int16BE(apiKey)\n    .Int16BE(apiVersion || API_VERSION)\n    .Int32BE(correlationId)\n    .Int16BE(clientId.length)\n    .string(clientId);\n}\n\nfunction encodeFetchRequest (maxWaitMs, minBytes) {\n  return function encodeFetchRequest (clientId, correlationId, payloads) {\n    return _encodeFetchRequest(clientId, correlationId, payloads, maxWaitMs, minBytes);\n  };\n}\n\nfunction decodeTopics (decodePartitions) {\n  return function (end, vars) {\n    if (--vars.topicNum === 0) end();\n    this.word16bs('topic')\n      .tap(function (vars) {\n        this.buffer('topic', vars.topic);\n        vars.topic = vars.topic.toString();\n      })\n      .word32bs('partitionNum')\n      .loop(decodePartitions);\n  };\n}\n\nfunction _encodeFetchRequest (clientId, correlationId, payloads, maxWaitMs, minBytes) {\n  payloads = groupByTopic(payloads);\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.fetch);\n  var topics = Object.keys(payloads);\n\n  request.Int32BE(REPLICA_ID)\n    .Int32BE(maxWaitMs)\n    .Int32BE(minBytes)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n\n    var partitions = _.toPairs(payloads[topic]).map(function (pairs) { return pairs[1]; });\n    request.Int32BE(partitions.length);\n    partitions.forEach(function (p) {\n      request.Int32BE(p.partition)\n        .Int64BE(p.offset)\n        .Int32BE(p.maxBytes);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeFetchResponse (cb, maxTickMessages) {\n  return function (resp) {\n    return _decodeFetchResponse(resp, cb, maxTickMessages);\n  };\n}\n\nfunction createGroupError (errorCode) {\n  if (errorCode == null || errorCode === 0) {\n    return null;\n  }\n\n  var error = ERROR_CODE[errorCode];\n  if (error in GROUP_ERROR) {\n    error = new GROUP_ERROR[error]('Kafka Error Code: ' + errorCode);\n  } else {\n    error = new Error(error);\n  }\n  error.errorCode = errorCode;\n  return error;\n}\n\nfunction _decodeFetchResponse (resp, cb, maxTickMessages) {\n  var topics = {};\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('topicNum')\n    .loop(decodeTopics(decodePartitions));\n\n  function decodePartitions (end, vars) {\n    if (--vars.partitionNum === 0) end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word32bs('partition')\n      .word16bs('errorCode')\n      .word64bs('highWaterOffset')\n      .word32bs('messageSetSize')\n      .tap(function (vars) {\n        this.buffer('messageSet', vars.messageSetSize);\n        if (vars.errorCode !== 0) {\n          return cb({ topic: vars.topic, partition: vars.partition, message: ERROR_CODE[vars.errorCode] });\n        }\n        var messageSet = decodeMessageSet(vars.topic, vars.partition, vars.messageSet, cb, maxTickMessages, vars.highWaterOffset);\n        if (messageSet.length) {\n          var offset = messageSet[messageSet.length - 1];\n          topics[vars.topic][vars.partition] = offset;\n          topics[vars.topic].highWaterOffset = vars.highWaterOffset;\n        }\n      });\n  }\n  cb && cb(null, 'done', topics);\n}\n\nfunction decodeMessageSet (topic, partition, messageSet, cb, maxTickMessages, highWaterOffset) {\n  var set = [];\n  var messageCount = 0;\n  while (messageSet.length > 0) {\n    var cur = 8 + 4 + 4 + 1 + 1 + 4 + 4;\n    Binary.parse(messageSet)\n      .word64bs('offset')\n      .word32bs('messageSize')\n      .tap(function (vars) {\n        if (vars.messageSize > (messageSet.length - 12)) {\n          vars.partial = true;\n        }\n      })\n      .word32bs('crc')\n      .word8bs('magicByte')\n      .word8bs('attributes')\n      .word32bs('key')\n      .tap(function (vars) {\n        if (vars.key === -1) return;\n        cur += vars.key;\n        this.buffer('key', vars.key);\n      })\n      .word32bs('value')\n      .tap(function (vars) {\n        if (vars.value !== -1) {\n          cur += vars.value;\n          this.buffer('value', vars.value);\n        } else {\n          vars.value = null;\n        }\n\n        if (!vars.partial && vars.offset !== null) {\n          messageCount++;\n          set.push(vars.offset);\n          if (!cb) return;\n          var codec = getCodec(vars.attributes);\n          if (!codec) {\n            return cb(null, 'message', {\n              topic: topic,\n              value: vars.value,\n              offset: vars.offset,\n              partition: partition,\n              highWaterOffset: highWaterOffset,\n              key: vars.key\n            });\n          }\n          codec.decode(vars.value, function (error, inlineMessageSet) {\n            if (error) return; // Not sure where to report this\n            decodeMessageSet(topic, partition, inlineMessageSet, cb, maxTickMessages);\n          });\n        }\n      });\n    // Defensive code around potential denial of service\n    if (maxTickMessages && messageCount > maxTickMessages) break;\n    messageSet = messageSet.slice(cur);\n  }\n  return set;\n}\n\nfunction encodeMetadataRequest (clientId, correlationId, topics) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.metadata);\n  request.Int32BE(topics.length);\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n  });\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeMetadataResponse (resp) {\n  var brokers = {};\n  var out = {};\n  var topics = {};\n  var errors = [];\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('brokerNum')\n    .loop(decodeBrokers)\n    .word32bs('topicNum')\n    .loop(_decodeTopics);\n\n  function decodeBrokers (end, vars) {\n    if (vars.brokerNum-- === 0) return end();\n    this.word32bs('nodeId')\n      .word16bs('host')\n      .tap(function (vars) {\n        this.buffer('host', vars.host);\n        vars.host = vars.host.toString();\n      })\n      .word32bs('port')\n      .tap(function (vars) {\n        brokers[vars.nodeId] = { nodeId: vars.nodeId, host: vars.host, port: vars.port };\n      });\n  }\n\n  function _decodeTopics (end, vars) {\n    if (vars.topicNum-- === 0) return end();\n    this.word16bs('topicError')\n      .word16bs('topic')\n      .tap(function (vars) {\n        this.buffer('topic', vars.topic);\n        vars.topic = vars.topic.toString();\n      })\n      .word32bs('partitionNum')\n      .tap(function (vars) {\n        if (vars.topicError !== 0) {\n          return errors.push(ERROR_CODE[vars.topicError]);\n        }\n        this.loop(decodePartitions);\n      });\n  }\n\n  function decodePartitions (end, vars) {\n    if (vars.partitionNum-- === 0) return end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word16bs('errorCode')\n      .word32bs('partition')\n      .word32bs('leader')\n      .word32bs('replicasNum')\n      .tap(function (vars) {\n        var buffer = this.buffer('replicas', vars.replicasNum * 4).vars.replicas;\n        this.vars.replicas = bufferToArray(vars.replicasNum, buffer);\n      })\n      .word32bs('isrNum')\n      .tap(function (vars) {\n        var buffer = this.buffer('isr', vars.isrNum * 4).vars.isr;\n        this.vars.isr = bufferToArray(vars.isrNum, buffer);\n        if (vars.errorCode === 0 || vars.errorCode === 9) {\n          topics[vars.topic][vars.partition] =\n            new PartitionMetadata(vars.topic, vars.partition, vars.leader, vars.replicas, vars.isr);\n        } else {\n          errors.push(ERROR_CODE[vars.errorCode]);\n        }\n      });\n  }\n\n  if (!_.isEmpty(errors)) out.error = errors;\n  out.metadata = topics;\n  return [brokers, out];\n}\n\nfunction bufferToArray (num, buffer) {\n  var ret = [];\n  for (var i = 0; i < num; i++) {\n    ret.push(Binary.parse(buffer).word32bs('r').vars.r);\n    buffer = buffer.slice(4);\n  }\n  return ret;\n}\n\nfunction encodeOffsetCommitRequest (group) {\n  return function (clientId, correlationId, payloads) {\n    return _encodeOffsetCommitRequest(clientId, correlationId, group, payloads);\n  };\n}\n\nfunction encodeOffsetCommitV2Request (clientId, correlationId, group, generationId, memberId, payloads) {\n  payloads = groupByTopic(payloads);\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.offsetCommit, 2);\n  var topics = Object.keys(payloads);\n\n  request.Int16BE(group.length).string(group)\n    .Int32BE(generationId)\n    .Int16BE(memberId.length).string(memberId)\n    .Int64BE(-1)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n\n    var partitions = _.toPairs(payloads[topic]).map(function (pairs) { return pairs[1]; });\n    request.Int32BE(partitions.length);\n    partitions.forEach(function (p) {\n      request.Int32BE(p.partition)\n        .Int64BE(p.offset)\n        .Int16BE(p.metadata.length)\n        .string(p.metadata);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction _encodeOffsetCommitRequest (clientId, correlationId, group, payloads) {\n  payloads = groupByTopic(payloads);\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.offsetCommit);\n  var topics = Object.keys(payloads);\n\n  request.Int16BE(group.length)\n    .string(group)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n\n    var partitions = _.toPairs(payloads[topic]).map(function (pairs) { return pairs[1]; });\n    request.Int32BE(partitions.length);\n    partitions.forEach(function (p) {\n      request.Int32BE(p.partition)\n        .Int64BE(p.offset)\n        .Int16BE(p.metadata.length)\n        .string(p.metadata);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeOffsetCommitResponse (resp) {\n  var topics = {};\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('topicNum')\n    .loop(decodeTopics(decodePartitions));\n\n  function decodePartitions (end, vars) {\n    if (--vars.partitionNum === 0) end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word32bs('partition')\n      .word16bs('errorcode')\n      .tap(function (vars) {\n        topics[vars.topic]['partition'] = vars.partition;\n        topics[vars.topic]['errorCode'] = vars.errorcode;\n      });\n  }\n  return topics;\n}\n\nfunction encodeProduceRequest (requireAcks, ackTimeoutMs) {\n  return function (clientId, correlationId, payloads) {\n    return _encodeProduceRequest(clientId, correlationId, payloads, requireAcks, ackTimeoutMs);\n  };\n}\n\nfunction _encodeProduceRequest (clientId, correlationId, payloads, requireAcks, ackTimeoutMs) {\n  payloads = groupByTopic(payloads);\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.produce);\n  var topics = Object.keys(payloads);\n  request.Int16BE(requireAcks)\n    .Int32BE(ackTimeoutMs)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n\n    var reqs = _.toPairs(payloads[topic]).map(function (pairs) { return pairs[1]; });\n    request.Int32BE(reqs.length);\n    reqs.forEach(function (p) {\n      var messageSet = encodeMessageSet(p.messages);\n      request.Int32BE(p.partition)\n        .Int32BE(messageSet.length)\n        .string(messageSet);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction encodeMessageSet (messageSet) {\n  var buffer = new Buffermaker();\n  messageSet.forEach(function (message) {\n    var msg = encodeMessage(message);\n    buffer.Int64BE(0)\n      .Int32BE(msg.length)\n      .string(msg);\n  });\n  return buffer.make();\n}\n\nfunction encodeMessage (message) {\n  var m = new Buffermaker()\n    .Int8(message.magic)\n    .Int8(message.attributes);\n\n  var key = message.key;\n  if (key) {\n    m.Int32BE(message.key.length);\n    m.string(message.key);\n  } else {\n    m.Int32BE(-1);\n  }\n\n  var value = message.value;\n\n  if (value !== null && value !== undefined) {\n    if (Buffer.isBuffer(value)) {\n      m.Int32BE(value.length);\n    } else {\n      if (typeof value !== 'string') value = value.toString();\n      m.Int32BE(Buffer.byteLength(value));\n    }\n    m.string(value);\n  } else {\n    m.Int32BE(-1);\n  }\n  m = m.make();\n  var crc = crc32.signed(m);\n  return new Buffermaker()\n    .Int32BE(crc)\n    .string(m)\n    .make();\n}\n\nfunction decodeProduceResponse (resp) {\n  var topics = {};\n  var error;\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('topicNum')\n    .loop(decodeTopics(decodePartitions));\n\n  function decodePartitions (end, vars) {\n    if (--vars.partitionNum === 0) end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word32bs('partition')\n      .word16bs('errorCode')\n      .word64bs('offset')\n      .tap(function (vars) {\n        if (vars.errorCode) {\n          error = new Error(ERROR_CODE[vars.errorCode]);\n        } else {\n          topics[vars.topic][vars.partition] = vars.offset;\n        }\n      });\n  }\n  return error || topics;\n}\n\nfunction encodeOffsetFetchRequest (group) {\n  return function (clientId, correlationId, payloads) {\n    return _encodeOffsetFetchRequest(clientId, correlationId, group, payloads);\n  };\n}\n\nfunction encodeOffsetFetchV1Request (clientId, correlationId, group, payloads) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.offsetFetch, 1);\n  var topics = Object.keys(payloads);\n\n  request.Int16BE(group.length)\n    .string(group)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length).string(topic)\n      .Int32BE(payloads[topic].length);\n\n    payloads[topic].forEach(function (p) {\n      request.Int32BE(p);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction _encodeOffsetFetchRequest (clientId, correlationId, group, payloads) {\n  payloads = groupByTopic(payloads);\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.offsetFetch);\n  var topics = Object.keys(payloads);\n\n  request.Int16BE(group.length)\n    .string(group)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n\n    var partitions = _.toPairs(payloads[topic]).map(function (pairs) { return pairs[1]; });\n    request.Int32BE(partitions.length);\n    partitions.forEach(function (p) {\n      request.Int32BE(p.partition);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeOffsetFetchResponse (resp) {\n  var topics = {};\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('topicNum')\n    .loop(decodeTopics(decodePartitions));\n\n  function decodePartitions (end, vars) {\n    if (--vars.partitionNum === 0) end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word32bs('partition')\n      .word64bs('offset')\n      .word16bs('metadata')\n      .tap(function (vars) {\n        if (vars.metadata === -1) {\n          return;\n        }\n\n        this.buffer('metadata', vars.metadata);\n      })\n      .word16bs('errorCode')\n      .tap(function (vars) {\n        topics[vars.topic][vars.partition] = vars.errorCode === 0 ? vars.offset : -1;\n      });\n  }\n  return topics;\n}\n\nfunction decodeOffsetFetchV1Response (resp) {\n  var topics = {};\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('topicNum')\n    .loop(decodeTopics(decodePartitions));\n\n  function decodePartitions (end, vars) {\n    if (--vars.partitionNum === 0) end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word32bs('partition')\n      .word64bs('offset')\n      .word16bs('metadata')\n      .tap(function (vars) {\n        if (vars.metadata === -1) {\n          return;\n        }\n\n        this.buffer('metadata', vars.metadata);\n      })\n      .word16bs('errorCode')\n      .tap(function (vars) {\n        if (vars.metadata.length === 0 && vars.offset === 0) {\n          topics[vars.topic][vars.partition] = -1;\n        } else {\n          topics[vars.topic][vars.partition] = vars.errorCode === 0 ? vars.offset : -1;\n        }\n      });\n  }\n  return topics;\n}\n\nfunction encodeOffsetRequest (clientId, correlationId, payloads) {\n  payloads = groupByTopic(payloads);\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.offset);\n  var topics = Object.keys(payloads);\n\n  request.Int32BE(REPLICA_ID)\n    .Int32BE(topics.length);\n\n  topics.forEach(function (topic) {\n    request.Int16BE(topic.length)\n      .string(topic);\n\n    var partitions = _.toPairs(payloads[topic]).map(function (pairs) { return pairs[1]; });\n    request.Int32BE(partitions.length);\n    partitions.forEach(function (p) {\n      request.Int32BE(p.partition)\n        .Int64BE(p.time)\n        .Int32BE(p.maxNum);\n    });\n  });\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeOffsetResponse (resp) {\n  var topics = {};\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word32bs('topicNum')\n    .loop(decodeTopics(decodePartitions));\n\n  function decodePartitions (end, vars) {\n    if (--vars.partitionNum === 0) end();\n    topics[vars.topic] = topics[vars.topic] || {};\n    this.word32bs('partition')\n      .word16bs('errorCode')\n      .word32bs('offsetNum')\n      .loop(decodeOffsets);\n  }\n\n  function decodeOffsets (end, vars) {\n    if (--vars.offsetNum <= 0) end();\n    topics[vars.topic][vars.partition] = topics[vars.topic][vars.partition] || [];\n    this.word64bs('offset')\n      .tap(function (vars) {\n        if (vars.offset != null) topics[vars.topic][vars.partition].push(vars.offset);\n      });\n  }\n  return topics;\n}\n\nfunction encodeGroupCoordinatorRequest (clientId, correlationId, groupId) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.groupCoordinator);\n  request.Int16BE(groupId.length).string(groupId);\n  return encodeRequestWithLength(request.make());\n}\n\nfunction encodeGroupHeartbeat (clientId, correlationId, groupId, generationId, memberId) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.heartbeat);\n  request\n    .Int16BE(groupId.length).string(groupId)\n    .Int32BE(generationId)\n    .Int16BE(memberId.length).string(memberId);\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeGroupHeartbeat (resp) {\n  var result = null;\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word16bs('errorCode')\n    .tap(function (vars) {\n      result = createGroupError(vars.errorCode);\n    });\n  return result;\n}\n\nfunction decodeGroupCoordinatorResponse (resp) {\n  var result;\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word16bs('errorCode')\n    .word32bs('coordinatorId')\n    .word16bs('coordinatorHost')\n    .tap(function (vars) {\n      this.buffer('coordinatorHost', vars.coordinatorHost);\n      vars.coordinatorHost = vars.coordinatorHost.toString();\n    })\n    .word32bs('coordinatorPort')\n    .tap(function (vars) {\n      if (vars.errorCode !== 0) {\n        result = createGroupError(vars.errorCode);\n        return;\n      }\n\n      result = {\n        coordinatorHost: vars.coordinatorHost,\n        coordinatorPort: vars.coordinatorPort,\n        coordinatorId: vars.coordinatorId\n      };\n    });\n  return result;\n}\n\n/*\n\nProtocolType => \"consumer\"\n\nProtocolName => AssignmentStrategy\n  AssignmentStrategy => string\n\nProtocolMetadata => Version Subscription UserData\n  Version => int16\n  Subscription => [Topic]\n    Topic => string\n  UserData => bytes\n*/\n\nfunction encodeGroupProtocol (protocol) {\n  this\n    .Int16BE(protocol.name.length).string(protocol.name)\n    .string(_encodeProtocolData(protocol));\n}\n\nfunction _encodeProtocolData (protocol) {\n  var protocolByte = new Buffermaker()\n    .Int16BE(protocol.version)\n    .Int32BE(protocol.subscription.length);\n  protocol.subscription.forEach(function (topic) {\n    protocolByte.Int16BE(topic.length).string(topic);\n  });\n\n  if (protocol.userData) {\n    var userDataStr = JSON.stringify(protocol.userData);\n    var data = new Buffer(userDataStr, 'utf8');\n    protocolByte.Int32BE(data.length).string(data);\n  } else {\n    protocolByte.Int32BE(-1);\n  }\n\n  return encodeRequestWithLength(protocolByte.make());\n}\n\nfunction decodeSyncGroupResponse (resp) {\n  var result;\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word16bs('errorCode')\n    .tap(function (vars) {\n      result = createGroupError(vars.errorCode);\n    })\n    .word32bs('memberAssignment')\n    .tap(function (vars) {\n      if (result) {\n        return;\n      }\n      this.buffer('memberAssignment', vars.memberAssignment);\n      result = decodeMemberAssignment(vars.memberAssignment);\n    });\n\n  return result;\n}\n\n/*\nMemberAssignment => Version PartitionAssignment\n  Version => int16\n  PartitionAssignment => [Topic [Partition]]\n    Topic => string\n    Partition => int32\n  UserData => bytes\n*/\n\nfunction decodeMemberAssignment (assignmentBytes) {\n  var assignment = {\n    partitions: {}\n  };\n\n  Binary.parse(assignmentBytes)\n    .word16bs('version')\n    .tap(function (vars) {\n      assignment.version = vars.version;\n    })\n    .word32bs('partitionAssignment')\n    .loop(function (end, vars) {\n      if (vars.partitionAssignment-- === 0) return end();\n\n      var topic;\n      var partitions = [];\n\n      this.word16bs('topic')\n        .tap(function (vars) {\n          this.buffer('topic', vars.topic);\n          topic = vars.topic.toString();\n        })\n        .word32bs('partitionsNum')\n        .loop(function (end, vars) {\n          if (vars.partitionsNum-- === 0) return end();\n          this.word32bs('partition').tap(function (vars) {\n            partitions.push(vars.partition);\n          });\n        });\n      assignment.partitions[topic] = partitions;\n    })\n    .word32bs('userData')\n    .tap(function (vars) {\n      if (vars.userData == null || vars.userData === -1) {\n        return;\n      }\n      this.buffer('userData', vars.userData);\n      try {\n        assignment.userData = JSON.parse(vars.userData.toString());\n      } catch (e) {\n        assignment.userData = 'JSON Parse error';\n      }\n    });\n\n  return assignment;\n}\n\nfunction encodeSyncGroupRequest (clientId, correlationId, groupId, generationId, memberId, groupAssignment) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.syncGroup);\n  request\n    .Int16BE(groupId.length).string(groupId)\n    .Int32BE(generationId)\n    .Int16BE(memberId.length).string(memberId);\n\n  if (groupAssignment && groupAssignment.length) {\n    request.Int32BE(groupAssignment.length);\n    groupAssignment.forEach(function (assignment) {\n      request.Int16BE(assignment.memberId.length).string(assignment.memberId)\n        .string(_encodeMemberAssignment(assignment));\n    });\n  } else {\n    request.Int32BE(0);\n  }\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction _encodeMemberAssignment (assignment) {\n  var numberOfTopics = Object.keys(assignment.topicPartitions).length;\n\n  var assignmentByte = new Buffermaker()\n    .Int16BE(assignment.version)\n    .Int32BE(numberOfTopics);\n\n  for (var tp in assignment.topicPartitions) {\n    if (!assignment.topicPartitions.hasOwnProperty(tp)) {\n      continue;\n    }\n    var partitions = assignment.topicPartitions[tp];\n    assignmentByte.Int16BE(tp.length).string(tp)\n      .Int32BE(partitions.length);\n\n    partitions.forEach(function (partition) {\n      assignmentByte.Int32BE(partition);\n    });\n  }\n\n  if (assignment.userData) {\n    var userDataStr = JSON.stringify(assignment.userData);\n    var data = new Buffer(userDataStr, 'utf8');\n    assignmentByte.Int32BE(data.length).string(data);\n  } else {\n    assignmentByte.Int32BE(-1);\n  }\n\n  return encodeRequestWithLength(assignmentByte.make());\n}\n\nfunction encodeLeaveGroupRequest (clientId, correlationId, groupId, memberId) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.leaveGroup);\n  request\n    .Int16BE(groupId.length).string(groupId)\n    .Int16BE(memberId.length).string(memberId);\n\n  return encodeRequestWithLength(request.make());\n}\n\nfunction decodeLeaveGroupResponse (resp) {\n  var error = null;\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word16bs('errorCode')\n    .tap(function (vars) {\n      error = createGroupError(vars.errorCode);\n    });\n  return error;\n}\n\n// {\n//   name: '', // string\n//   subscription: [/* topics */],\n//   version: 0, // integer\n//   userData: {} //arbitary\n// }\n\n/*\nJoinGroupRequest => GroupId SessionTimeout MemberId ProtocolType GroupProtocols\n  GroupId => string\n  SessionTimeout => int32\n  MemberId => string\n  ProtocolType => string\n  GroupProtocols => [ProtocolName ProtocolMetadata]\n    ProtocolName => string\n    ProtocolMetadata => bytes\n*/\n\nfunction encodeJoinGroupRequest (clientId, correlationId, groupId, memberId, sessionTimeout, groupProtocols) {\n  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.joinGroup);\n  request\n    .Int16BE(groupId.length).string(groupId)\n    .Int32BE(sessionTimeout)\n    .Int16BE(memberId.length).string(memberId)\n    .Int16BE(GROUPS_PROTOCOL_TYPE.length).string(GROUPS_PROTOCOL_TYPE)\n    .Int32BE(groupProtocols.length);\n\n  groupProtocols.forEach(encodeGroupProtocol.bind(request));\n\n  return encodeRequestWithLength(request.make());\n}\n\n/*\n\nv0 and v1 supported in 0.9.0 and greater\nJoinGroupResponse => ErrorCode GenerationId GroupProtocol LeaderId MemberId Members\n  ErrorCode => int16\n  GenerationId => int32\n  GroupProtocol => string\n  LeaderId => string\n  MemberId => string\n  Members => [MemberId MemberMetadata]\n    MemberId => string\n    MemberMetadata => bytes\n*/\nfunction decodeJoinGroupResponse (resp) {\n  var result = {\n    members: []\n  };\n\n  var error;\n\n  Binary.parse(resp)\n    .word32bs('size')\n    .word32bs('correlationId')\n    .word16bs('errorCode')\n    .tap(function (vars) {\n      error = createGroupError(vars.errorCode);\n    })\n    .word32bs('generationId')\n    .tap(function (vars) {\n      result.generationId = vars.generationId;\n    })\n    .word16bs('groupProtocol')\n    .tap(function (vars) {\n      this.buffer('groupProtocol', vars.groupProtocol);\n      result.groupProtocol = vars.groupProtocol = vars.groupProtocol.toString();\n    })\n    .word16bs('leaderId')\n    .tap(function (vars) {\n      this.buffer('leaderId', vars.leaderId);\n      result.leaderId = vars.leaderId = vars.leaderId.toString();\n    })\n    .word16bs('memberId')\n    .tap(function (vars) {\n      this.buffer('memberId', vars.memberId);\n      result.memberId = vars.memberId = vars.memberId.toString();\n    })\n    .word32bs('memberNum')\n    .loop(function (end, vars) {\n      if (error) {\n        return end();\n      }\n\n      if (vars.memberNum-- === 0) return end();\n      var memberMetadata;\n      this\n        .word16bs('groupMemberId').tap(function (vars) {\n          this.buffer('groupMemberId', vars.groupMemberId);\n          vars.memberId = vars.groupMemberId.toString();\n        })\n        .word32bs('memberMetadata').tap(function (vars) {\n          if (vars.memberMetadata > -1) {\n            this.buffer('memberMetadata', vars.memberMetadata);\n            memberMetadata = decodeGroupData(this.vars.memberMetadata);\n            memberMetadata.id = vars.memberId;\n            result.members.push(memberMetadata);\n          }\n        });\n    });\n\n  return error || result;\n}\n\nfunction decodeGroupData (resp) {\n  var topics = [];\n  var parsed = Binary.parse(resp)\n    .word16bs('version')\n    .word32bs('subscriptionNum')\n    .loop(function decodeSubscription (end, vars) {\n      if (vars.subscriptionNum-- === 0) return end();\n      this.word16bs('topic').tap(function () {\n        this.buffer('topic', vars.topic);\n        topics.push(vars.topic.toString());\n      });\n    })\n    .word32bs('userData')\n    .tap(function (vars) {\n      if (vars.userData === -1) {\n        vars.userData = undefined;\n        return;\n      }\n      this.buffer('userData', vars.userData);\n      try {\n        vars.userData = JSON.parse(vars.userData.toString());\n      } catch (error) {\n        vars.userData = 'JSON parse error';\n      }\n    })\n    .vars;\n\n  return {\n    subscription: topics,\n    version: parsed.version,\n    userData: parsed.userData\n  };\n}\n\nexports.encodeFetchRequest = encodeFetchRequest;\nexports.decodeFetchResponse = decodeFetchResponse;\nexports.encodeOffsetCommitRequest = encodeOffsetCommitRequest;\nexports.encodeOffsetCommitV2Request = encodeOffsetCommitV2Request;\nexports.decodeOffsetCommitResponse = decodeOffsetCommitResponse;\nexports.encodeOffsetFetchRequest = encodeOffsetFetchRequest;\nexports.encodeOffsetFetchV1Request = encodeOffsetFetchV1Request;\nexports.decodeOffsetFetchResponse = decodeOffsetFetchResponse;\nexports.decodeOffsetFetchV1Response = decodeOffsetFetchV1Response;\nexports.encodeMetadataRequest = encodeMetadataRequest;\nexports.decodeMetadataResponse = decodeMetadataResponse;\nexports.encodeProduceRequest = encodeProduceRequest;\nexports.decodeProduceResponse = decodeProduceResponse;\nexports.encodeOffsetRequest = encodeOffsetRequest;\nexports.decodeOffsetResponse = decodeOffsetResponse;\nexports.encodeMessageSet = encodeMessageSet;\nexports.encodeJoinGroupRequest = encodeJoinGroupRequest;\nexports.decodeJoinGroupResponse = decodeJoinGroupResponse;\nexports.encodeGroupCoordinatorRequest = encodeGroupCoordinatorRequest;\nexports.decodeGroupCoordinatorResponse = decodeGroupCoordinatorResponse;\nexports.encodeGroupHeartbeat = encodeGroupHeartbeat;\nexports.decodeGroupHeartbeat = decodeGroupHeartbeat;\nexports.encodeSyncGroupRequest = encodeSyncGroupRequest;\nexports.decodeSyncGroupResponse = decodeSyncGroupResponse;\nexports.encodeLeaveGroupRequest = encodeLeaveGroupRequest;\nexports.decodeLeaveGroupResponse = decodeLeaveGroupResponse;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/codec/index.js":"'use strict';\n\nvar zlib = require('zlib');\nvar snappyCodec = require('./snappy');\n\nvar gzipCodec = {\n  encode: zlib.gzip,\n  decode: zlib.gunzip\n};\n\nvar codecs = [\n  null,\n  gzipCodec,\n  snappyCodec\n];\n\nfunction getCodec (attributes) {\n  return codecs[attributes & 3] || null;\n}\n\nmodule.exports = getCodec;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/codec/snappy.js":"'use strict';\n\nvar optional = require('optional');\nvar async = require('async');\nvar snappy = optional('snappy');\n\nif (snappy == null) {\n  var unavailableCodec = function unavailableCodec () {\n    throw new Error('Snappy codec is not installed');\n  };\n  module.exports = {\n    encode: unavailableCodec,\n    decode: unavailableCodec\n  };\n} else {\n  var SNAPPY_MAGIC_BYTES = [130, 83, 78, 65, 80, 80, 89, 0];\n  var SNAPPY_MAGIC_BYTES_LEN = SNAPPY_MAGIC_BYTES.length;\n  var SNAPPY_MAGIC = new Buffer(SNAPPY_MAGIC_BYTES).toString('hex');\n\n  exports.encode = snappy.compress;\n  exports.decode = decodeSnappy;\n}\n\nfunction isChunked (buffer) {\n  var prefix = buffer.toString('hex', 0, SNAPPY_MAGIC_BYTES_LEN);\n  return prefix === SNAPPY_MAGIC;\n}\n\n// Ported from:\n// https://github.com/Shopify/sarama/blob/a3e2437d6d26cda6b2dc501dbdab4d3f6befa295/snappy.go\nfunction decodeSnappy (buffer, cb) {\n  if (isChunked(buffer)) {\n    var pos = 16;\n    var max = buffer.length;\n    var encoded = [];\n    var size;\n\n    while (pos < max) {\n      size = buffer.readUInt32BE(pos);\n      pos += 4;\n      encoded.push(buffer.slice(pos, pos + size));\n      pos += size;\n    }\n    return async.mapSeries(encoded, snappy.uncompress,\n      function (err, decodedChunks) {\n        if (err) return cb(err);\n        return cb(null, Buffer.concat(decodedChunks));\n      }\n    );\n  }\n  return snappy.uncompress(buffer, cb);\n}\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/partitioner.js":"'use strict';\n\nvar util = require('util');\nvar _ = require('lodash');\n\nvar Partitioner = function () {};\n\nvar DefaultPartitioner = function () {};\nutil.inherits(DefaultPartitioner, Partitioner);\n\nDefaultPartitioner.prototype.getPartition = function (partitions) {\n  if (partitions && _.isArray(partitions) && partitions.length > 0) {\n    return partitions[0];\n  } else {\n    return 0;\n  }\n};\n\nvar CyclicPartitioner = function () {\n  this.c = 0;\n};\nutil.inherits(CyclicPartitioner, Partitioner);\n\nCyclicPartitioner.prototype.getPartition = function (partitions) {\n  if (_.isEmpty(partitions)) return 0;\n  return partitions[ this.c++ % partitions.length ];\n};\n\nvar RandomPartitioner = function () {};\nutil.inherits(RandomPartitioner, Partitioner);\n\nRandomPartitioner.prototype.getPartition = function (partitions) {\n  return partitions[Math.floor(Math.random() * partitions.length)];\n};\n\nvar KeyedPartitioner = function () {};\nutil.inherits(KeyedPartitioner, Partitioner);\n\n// Taken from oid package (Dan Bornstein)\n// Copyright The Obvious Corporation.\nKeyedPartitioner.prototype.hashCode = function (string) {\n  var hash = 0;\n  var length = string.length;\n\n  for (var i = 0; i < length; i++) {\n    hash = ((hash * 31) + string.charCodeAt(i)) & 0x7fffffff;\n  }\n\n  return (hash === 0) ? 1 : hash;\n};\n\nKeyedPartitioner.prototype.getPartition = function (partitions, key) {\n  key = key || '';\n\n  var index = this.hashCode(key) % partitions.length;\n  return partitions[index];\n};\n\nvar CustomPartitioner = function (partitioner) {\n  this.getPartition = partitioner;\n};\nutil.inherits(CustomPartitioner, Partitioner);\n\nmodule.exports.DefaultPartitioner = DefaultPartitioner;\nmodule.exports.CyclicPartitioner = CyclicPartitioner;\nmodule.exports.RandomPartitioner = RandomPartitioner;\nmodule.exports.KeyedPartitioner = KeyedPartitioner;\nmodule.exports.CustomPartitioner = CustomPartitioner;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/consumerGroup.js":"'use strict';\n\nconst logger = require('./logging')('kafka-node:ConsumerGroup');\nconst util = require('util');\nconst EventEmitter = require('events');\nconst highLevelConsumer = require('./highLevelConsumer');\nconst Client = require('./client');\nconst Offset = require('./offset');\nconst _ = require('lodash');\nconst async = require('async');\nconst validateConfig = require('./utils').validateConfig;\nconst ConsumerGroupRecovery = require('./consumerGroupRecovery');\nconst Heartbeat = require('./consumerGroupHeartbeat');\nconst createTopicPartitionList = require('./utils').createTopicPartitionList;\nconst errors = require('./errors');\n\nconst assert = require('assert');\nconst builtInProtocols = require('./assignment');\n\nconst LATEST_OFFSET = -1;\nconst EARLIEST_OFFSET = -2;\nconst ACCEPTED_FROM_OFFSET = {\n  latest: LATEST_OFFSET,\n  earliest: EARLIEST_OFFSET,\n  none: false\n};\n\nconst DEFAULTS = {\n  groupId: 'kafka-node-group',\n  // Auto commit config\n  autoCommit: true,\n  autoCommitIntervalMs: 5000,\n  // Fetch message config\n  fetchMaxWaitMs: 100,\n  paused: false,\n  maxNumSegments: 1000,\n  fetchMinBytes: 1,\n  fetchMaxBytes: 1024 * 1024,\n  maxTickMessages: 1000,\n  fromOffset: 'latest',\n  outOfRangeOffset: 'earliest',\n  sessionTimeout: 30000,\n  retries: 10,\n  retryFactor: 1.8,\n  retryMinTimeout: 1000,\n  connectOnReady: true,\n  migrateHLC: false,\n  migrateRolling: true,\n  protocol: ['roundrobin']\n};\n\nfunction ConsumerGroup (memberOptions, topics) {\n  EventEmitter.call(this);\n  const self = this;\n  this.options = _.defaults((memberOptions || {}), DEFAULTS);\n\n  if (!this.options.heartbeatInterval) {\n    this.options.heartbeatInterval = Math.floor(this.options.sessionTimeout / 3);\n  }\n\n  if (memberOptions.ssl === true) {\n    memberOptions.ssl = {};\n  }\n\n  if (!(this.options.fromOffset in ACCEPTED_FROM_OFFSET)) {\n    throw new Error(`fromOffset ${this.options.fromOffset} should be either: ${Object.keys(ACCEPTED_FROM_OFFSET).join(', ')}`);\n  }\n\n  if (!(this.options.outOfRangeOffset in ACCEPTED_FROM_OFFSET)) {\n    throw new Error(`outOfRangeOffset ${this.options.outOfRangeOffset} should be either: ${Object.keys(ACCEPTED_FROM_OFFSET).join(', ')}`);\n  }\n\n  this.client = new Client(memberOptions.host, memberOptions.id, memberOptions.zk,\n    memberOptions.batch, memberOptions.ssl);\n\n  if (_.isString(topics)) {\n    topics = [topics];\n  }\n\n  assert(Array.isArray(topics), 'Array of topics is required');\n\n  this.topics = topics;\n\n  this.recovery = new ConsumerGroupRecovery(this);\n\n  this.setupProtocols(this.options.protocol);\n\n  if (this.options.connectOnReady && !this.options.migrateHLC) {\n    this.client.once('ready', this.connect.bind(this));\n  }\n\n  if (this.options.migrateHLC) {\n    const ConsumerGroupMigrator = require('./consumerGroupMigrator');\n    this.migrator = new ConsumerGroupMigrator(this);\n    this.migrator.on('error', function (error) {\n      self.emit('error', error);\n    });\n  }\n\n  this.client.on('error', function (err) {\n    logger.error('Error from %s', self.client.clientId, err);\n    self.emit('error', err);\n  });\n\n  const recoverFromBrokerChange = _.debounce(function () {\n    logger.debug('brokersChanged refreshing metadata');\n    self.client.refreshMetadata(self.topics, function (error) {\n      if (error) {\n        self.emit(error);\n        return;\n      }\n      self.paused = false;\n      if (!self.ready && !self.connecting) {\n        if (self.reconnectTimer) {\n          // brokers changed so bypass backoff retry and reconnect now\n          clearTimeout(self.reconnectTimer);\n          self.reconnectTimer = null;\n        }\n        self.connect();\n      } else if (!self.connecting) {\n        self.fetch();\n      }\n    });\n  }, 200);\n\n  this.client.on('brokersChanged', function () {\n    self.pause();\n    recoverFromBrokerChange();\n  });\n\n  this.client.on('reconnect', function (lastError) {\n    self.fetch();\n  });\n\n  this.on('offsetOutOfRange', topic => {\n    this.pause();\n    if (this.options.outOfRangeOffset === 'none') {\n      this.emit('error', new errors.InvalidConsumerOffsetError(`Offset out of range for topic \"${topic.topic}\" partition ${topic.partition}`));\n      return;\n    }\n\n    topic.time = ACCEPTED_FROM_OFFSET[this.options.outOfRangeOffset];\n\n    this.getOffset().fetch([topic], (error, result) => {\n      if (error) {\n        this.emit('error', new errors.InvalidConsumerOffsetError(`Fetching ${this.options.outOfRangeOffset} offset failed`, error));\n        return;\n      }\n      const offset = _.head(result[topic.topic][topic.partition]);\n      const oldOffset = _.find(this.topicPayloads, {topic: topic.topic, partition: topic.partition}).offset;\n\n      logger.debug('replacing %s-%s stale offset of %d with %d', topic.topic, topic.partition, oldOffset, offset);\n\n      this.setOffset(topic.topic, topic.partition, offset);\n      this.resume();\n    });\n  });\n\n  // 'done' will be emit when a message fetch request complete\n  this.on('done', function (topics) {\n    self.updateOffsets(topics);\n    if (!self.paused) {\n      setImmediate(function () {\n        self.fetch();\n      });\n    }\n  });\n\n  if (this.options.groupId) {\n    validateConfig('options.groupId', this.options.groupId);\n  }\n\n  this.isLeader = false;\n  this.coordinatorId = null;\n  this.generationId = null;\n  this.ready = false;\n  this.topicPayloads = [];\n}\n\nutil.inherits(ConsumerGroup, highLevelConsumer);\n\nConsumerGroup.prototype.setupProtocols = function (protocols) {\n  if (!Array.isArray(protocols)) {\n    protocols = [protocols];\n  }\n\n  this.protocols = protocols.map(function (protocol) {\n    if (typeof protocol === 'string') {\n      if (!(protocol in builtInProtocols)) {\n        throw new Error('Unknown built in assignment protocol ' + protocol);\n      }\n      protocol = _.assign({}, builtInProtocols[protocol]);\n    } else {\n      checkProtocol(protocol);\n    }\n\n    protocol.subscription = this.topics;\n    return protocol;\n  }, this);\n};\n\nfunction checkProtocol (protocol) {\n  assert(protocol, 'protocol is null');\n  assert(protocol.assign, 'assign function is not defined in the protocol');\n  assert(protocol.name, 'name must be given to protocol');\n  assert(protocol.version >= 0, 'version must be >= 0');\n}\n\nConsumerGroup.prototype.setCoordinatorId = function (coordinatorId) {\n  this.client.coordinatorId = String(coordinatorId);\n};\n\nConsumerGroup.prototype.assignPartitions = function (protocol, groupMembers, callback) {\n  logger.debug('Assigning Partitions to members', groupMembers);\n  logger.debug('Using group protocol', protocol);\n\n  protocol = _.find(this.protocols, {name: protocol});\n\n  var self = this;\n  var topics = _(groupMembers).map('subscription').flatten().uniq().value();\n\n  async.waterfall([\n    function (callback) {\n      logger.debug('loadingMetadata for topics:', topics);\n      self.client.loadMetadataForTopics(topics, callback);\n    },\n\n    function (metadataResponse, callback) {\n      var metadata = mapTopicToPartitions(metadataResponse[1].metadata);\n      logger.debug('mapTopicToPartitions', metadata);\n      protocol.assign(metadata, groupMembers, callback);\n    }\n  ], callback);\n};\n\nfunction mapTopicToPartitions (metadata) {\n  return _.mapValues(metadata, Object.keys);\n}\n\nConsumerGroup.prototype.handleJoinGroup = function (joinGroupResponse, callback) {\n  logger.debug('joinGroupResponse %j from %s', joinGroupResponse, this.client.clientId);\n\n  this.isLeader = (joinGroupResponse.leaderId === joinGroupResponse.memberId);\n  this.generationId = joinGroupResponse.generationId;\n  this.memberId = joinGroupResponse.memberId;\n\n  var groupAssignment;\n  if (this.isLeader) {\n    // assign partitions\n    return this.assignPartitions(joinGroupResponse.groupProtocol, joinGroupResponse.members, callback);\n  }\n  callback(null, groupAssignment);\n};\n\nConsumerGroup.prototype.saveDefaultOffsets = function (topicPartitionList, callback) {\n  var self = this;\n  const offsetPayload = _(topicPartitionList).cloneDeep().map(tp => {\n    tp.time = ACCEPTED_FROM_OFFSET[this.options.fromOffset];\n    return tp;\n  });\n\n  self.getOffset().fetch(offsetPayload, function (error, result) {\n    if (error) {\n      return callback(error);\n    }\n    self.defaultOffsets = _.mapValues(result, function (partitionOffsets) {\n      return _.mapValues(partitionOffsets, _.head);\n    });\n    callback(null);\n  });\n};\n\nConsumerGroup.prototype.handleSyncGroup = function (syncGroupResponse, callback) {\n  logger.debug('SyncGroup Response');\n  var self = this;\n  var ownedTopics = Object.keys(syncGroupResponse.partitions);\n  if (ownedTopics.length) {\n    logger.debug('%s owns topics: ', self.client.clientId, syncGroupResponse.partitions);\n\n    const topicPartitionList = createTopicPartitionList(syncGroupResponse.partitions);\n    const useDefaultOffsets = self.options.fromOffset in ACCEPTED_FROM_OFFSET;\n\n    async.waterfall([\n      function (callback) {\n        self.fetchOffset(syncGroupResponse.partitions, callback);\n      },\n      function (offsets, callback) {\n        logger.debug('%s fetchOffset Response: %j', self.client.clientId, offsets);\n\n        var noOffset = topicPartitionList.some(function (tp) {\n          return offsets[tp.topic][tp.partition] === -1;\n        });\n\n        if (noOffset) {\n          logger.debug('No saved offsets');\n\n          if (self.options.fromOffset === 'none') {\n            return callback(new Error(`${self.client.clientId} owns topics and partitions which contains no saved offsets for group '${self.options.groupId}'`));\n          }\n\n          async.parallel([\n            function (callback) {\n              if (self.migrator) {\n                return self.migrator.saveHighLevelConsumerOffsets(topicPartitionList, callback);\n              }\n              callback(null);\n            },\n            function (callback) {\n              if (useDefaultOffsets) {\n                return self.saveDefaultOffsets(topicPartitionList, callback);\n              }\n              callback(null);\n            }\n          ], function (error) {\n            if (error) {\n              return callback(error);\n            }\n            logger.debug('%s defaultOffset Response for %s: %j', self.client.clientId, self.options.fromOffset, self.defaultOffsets);\n            callback(null, offsets);\n          });\n        } else {\n          logger.debug('Has saved offsets');\n          callback(null, offsets);\n        }\n      },\n      function (offsets, callback) {\n        self.topicPayloads = self.buildPayloads(topicPartitionList).map(function (p) {\n          var offset = offsets[p.topic][p.partition];\n          if (offset === -1) { // -1 means no offset was saved for this topic/partition combo\n            offset = useDefaultOffsets ? self.getDefaultOffset(p, 0) : 0;\n            if (self.migrator) {\n              offset = self.migrator.getOffset(p, offset);\n            }\n          }\n          p.offset = offset;\n          return p;\n        });\n        callback(null, true);\n      }\n    ], callback);\n  } else { // no partitions assigned\n    callback(null, false);\n  }\n};\n\nConsumerGroup.prototype.getDefaultOffset = function (tp, defaultOffset) {\n  return _.get(this.defaultOffsets, [tp.topic, tp.partition], defaultOffset);\n};\n\nConsumerGroup.prototype.getOffset = function () {\n  if (this.offset) {\n    return this.offset;\n  }\n  this.offset = new Offset(this.client);\n  // we can ignore this since we are already forwarding error event emitted from client\n  this.offset.on('error', _.noop);\n  return this.offset;\n};\n\nfunction emptyStrIfNull (value) {\n  return value == null ? '' : value;\n}\n\nConsumerGroup.prototype.connect = function () {\n  if (this.connecting) {\n    logger.warn('Connect ignored. Currently connecting.');\n    return;\n  }\n\n  logger.debug('Connecting %s', this.client.clientId);\n  var self = this;\n\n  this.connecting = true;\n  this.emit('rebalancing');\n\n  async.waterfall([\n    function (callback) {\n      if (self.client.coordinatorId) {\n        return callback(null, null);\n      }\n      self.client.sendGroupCoordinatorRequest(self.options.groupId, callback);\n    },\n\n    function (coordinatorInfo, callback) {\n      logger.debug('GroupCoordinator Response:', coordinatorInfo);\n      if (coordinatorInfo) {\n        self.setCoordinatorId(coordinatorInfo.coordinatorId);\n      }\n      self.client.sendJoinGroupRequest(self.options.groupId, emptyStrIfNull(self.memberId), self.options.sessionTimeout, self.protocols, callback);\n    },\n\n    function (joinGroupResponse, callback) {\n      self.handleJoinGroup(joinGroupResponse, callback);\n    },\n\n    function (groupAssignment, callback) {\n      logger.debug('SyncGroup Request from %s', self.memberId);\n      self.client.sendSyncGroupRequest(self.options.groupId, self.generationId, self.memberId, groupAssignment, callback);\n    },\n\n    function (syncGroupResponse, callback) {\n      self.handleSyncGroup(syncGroupResponse, callback);\n    }\n  ], function (error, startFetch) {\n    self.connecting = false;\n    self.rebalancing = false;\n    if (error) {\n      return self.recovery.tryToRecoverFrom(error, 'connect');\n    }\n\n    self.ready = true;\n    self.recovery.clearError();\n\n    logger.debug('generationId', self.generationId);\n\n    if (startFetch) {\n      self.fetch();\n    }\n    self.startHeartbeats();\n    self.emit('connect');\n    self.emit('rebalanced');\n  });\n};\n\nConsumerGroup.prototype.scheduleReconnect = function (timeout) {\n  assert(timeout);\n  this.rebalancing = true;\n\n  if (this.reconnectTimer) {\n    clearTimeout(this.reconnectTimer);\n  }\n\n  var self = this;\n  this.reconnectTimer = setTimeout(function () {\n    self.reconnectTimer = null;\n    self.connect();\n  }, timeout);\n};\n\nConsumerGroup.prototype.startHeartbeats = function () {\n  assert(this.options.sessionTimeout > 0);\n  assert(this.ready, 'consumerGroup is not ready');\n\n  const heartbeatIntervalMs = this.options.heartbeatInterval || (Math.floor(this.options.sessionTimeout / 3));\n\n  logger.debug('%s started heartbeats at every %d ms', this.client.clientId, heartbeatIntervalMs);\n  this.stopHeartbeats();\n\n  let heartbeat = this.sendHeartbeat();\n\n  this.heartbeatInterval = setInterval(() => {\n    // only send another heartbeat if we got a response from the last one\n    if (heartbeat.verifyResolved()) {\n      heartbeat = this.sendHeartbeat();\n    }\n  }, heartbeatIntervalMs);\n};\n\nConsumerGroup.prototype.stopHeartbeats = function () {\n  this.heartbeatInterval && clearInterval(this.heartbeatInterval);\n};\n\nConsumerGroup.prototype.leaveGroup = function (callback) {\n  logger.debug('%s leaving group', this.client.clientId);\n  var self = this;\n  this.stopHeartbeats();\n  if (self.generationId != null && self.memberId) {\n    this.client.sendLeaveGroupRequest(this.options.groupId, this.memberId, function (error) {\n      self.generationId = null;\n      callback(error);\n    });\n  } else {\n    callback(null);\n  }\n};\n\nConsumerGroup.prototype.sendHeartbeat = function () {\n  assert(this.memberId, 'invalid memberId');\n  assert(this.generationId >= 0, 'invalid generationId');\n  // logger.debug('%s ❤️  ->', this.client.clientId);\n  var self = this;\n\n  function heartbeatCallback (error) {\n    if (error) {\n      logger.warn('%s Heartbeat error:', self.client.clientId, error);\n      self.recovery.tryToRecoverFrom(error, 'heartbeat');\n    }\n    // logger.debug('%s 💚 <-', self.client.clientId, error);\n  }\n\n  const heartbeat = new Heartbeat(this.client, heartbeatCallback);\n  heartbeat.send(this.options.groupId, this.generationId, this.memberId);\n\n  return heartbeat;\n};\n\nConsumerGroup.prototype.fetchOffset = function (payloads, cb) {\n  this.client.sendOffsetFetchV1Request(this.options.groupId, payloads, cb);\n};\n\nConsumerGroup.prototype.sendOffsetCommitRequest = function (commits, cb) {\n  if (this.generationId && this.memberId) {\n    this.client.sendOffsetCommitV2Request(this.options.groupId, this.generationId, this.memberId, commits, cb);\n  } else {\n    cb(null, 'Nothing to be committed');\n  }\n};\n\nConsumerGroup.prototype.close = function (force, cb) {\n  var self = this;\n  this.ready = false;\n\n  this.stopHeartbeats();\n\n  if (typeof force === 'function') {\n    cb = force;\n    force = false;\n  }\n\n  async.series([\n    function (callback) {\n      if (force) {\n        self.commit(true, callback);\n        return;\n      }\n      callback(null);\n    },\n    function (callback) {\n      self.leaveGroup(function (error) {\n        if (error) {\n          logger.error('Leave group failed with', error);\n        }\n        callback(null);\n      });\n    },\n    function (callback) {\n      self.client.close(callback);\n    }\n  ], cb);\n};\n\nmodule.exports = ConsumerGroup;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/client.js":"'use strict';\n\nvar net = require('net');\nvar assert = require('assert');\nvar tls = require('tls');\nvar util = require('util');\nvar _ = require('lodash');\nvar async = require('async');\nvar retry = require('retry');\nvar events = require('events');\nvar errors = require('./errors');\nvar Binary = require('binary');\nvar getCodec = require('./codec');\nvar protocol = require('./protocol');\nvar BrokerWrapper = require('./wrapper/BrokerWrapper');\nvar encodeMessageSet = protocol.encodeMessageSet;\nvar Message = protocol.Message;\nvar zk = require('./zookeeper');\nvar Zookeeper = zk.Zookeeper;\nvar url = require('url');\nvar logger = require('./logging')('kafka-node:Client');\nvar validateConfig = require('./utils').validateConfig;\nvar validateKafkaTopics = require('./utils').validateTopicNames;\n\nconst MAX_INT32 = 2147483647;\n\n/**\n * Communicates with kafka brokers\n * Uses zookeeper to discover all the kafka brokers to connect to\n *\n * @example <caption>Non chrooted connection to a single zookeeper host</caption>\n * var client = new Client('localhost:2181')\n *\n * @example <caption>Chrooted connection to multiple zookeeper hosts</caption>\n * var client = new Client('localhost:2181,localhost:2182/exmaple/chroot\n *\n * @param {String} [connectionString='localhost:2181/kafka0.8'] A string containing a list of zookeeper hosts:port\n *      and the zookeeper chroot\n * @param {String} [clientId='kafka-node-client'] The client id to register with zookeeper, helpful for debugging\n * @param {Object} zkOptions Pass through options to the zookeeper client library\n *\n * @param {Object} noAckBatchOptions Batch buffer options for no ACK requirement producers\n * - noAckBatchOptions.noAckBatchSize Max batch size in bytes for the buffer before sending all data to broker\n * - noAckBatchOptions.noAckBatchAge Timeout max for the buffer to retain data before sending all data to broker\n *\n * @param {Object} sslOptions options for TLS Socket\n *\n * @constructor\n */\nvar Client = function (connectionString, clientId, zkOptions, noAckBatchOptions, sslOptions) {\n  if (this instanceof Client === false) {\n    return new Client(connectionString, clientId, zkOptions, noAckBatchOptions, sslOptions);\n  }\n\n  this.sslOptions = sslOptions;\n  this.ssl = !!sslOptions;\n\n  if (clientId) {\n    validateConfig('clientId', clientId);\n  }\n\n  this.connectionString = connectionString || 'localhost:2181/';\n  this.clientId = clientId || 'kafka-node-client';\n  this.zkOptions = zkOptions;\n  this.noAckBatchOptions = noAckBatchOptions;\n  this.brokers = {};\n  this.longpollingBrokers = {};\n  this.topicMetadata = {};\n  this.topicPartitions = {};\n  this.correlationId = 0;\n  this._socketId = 0;\n  this.cbqueue = {};\n  this.brokerMetadata = {};\n  this.ready = false;\n  this.connect();\n};\n\nutil.inherits(Client, events.EventEmitter);\n\nClient.prototype.connect = function () {\n  var zk = this.zk = new Zookeeper(this.connectionString, this.zkOptions);\n  var self = this;\n  zk.once('init', function (brokers) {\n    try {\n      self.ready = true;\n      self.brokerMetadata = brokers;\n      self.setupBrokerProfiles(brokers);\n      Object\n          .keys(self.brokerProfiles)\n          .some(function (key, index) {\n            var broker = self.brokerProfiles[key];\n            self.setupBroker(broker.host, broker.port, false, self.brokers);\n            // Only connect one broker\n            return !index;\n          });\n      self.emit('ready');\n    } catch (error) {\n      self.ready = false;\n      self.emit('error', error);\n    }\n  });\n  zk.on('brokersChanged', function (brokerMetadata) {\n    try {\n      self.brokerMetadata = brokerMetadata;\n      logger.debug('brokersChanged', brokerMetadata);\n      self.setupBrokerProfiles(brokerMetadata);\n      self.refreshBrokers();\n      // Emit after a 3 seconds\n      setTimeout(function () {\n        self.emit('brokersChanged');\n      }, 3000);\n    } catch (error) {\n      self.emit('error', error);\n    }\n  });\n  zk.once('disconnected', function () {\n    if (!zk.closed) {\n      zk.close();\n      self.connect();\n      self.emit('zkReconnect');\n    }\n  });\n  zk.on('error', function (err) {\n    self.emit('error', err);\n  });\n};\n\nClient.prototype.setupBrokerProfiles = function (brokers) {\n  this.brokerProfiles = Object.create(null);\n  var self = this;\n  var protocol = self.ssl ? 'ssl:' : 'plaintext:';\n\n  Object.keys(brokers).forEach(function (key) {\n    var brokerProfile = brokers[key];\n    var addr;\n\n    if (brokerProfile.endpoints && brokerProfile.endpoints.length) {\n      var endpoint = _.find(brokerProfile.endpoints, function (endpoint) {\n        return url.parse(endpoint).protocol === protocol;\n      });\n\n      if (endpoint == null) {\n        throw new Error(['No kafka endpoint found for broker: ', key, ' with protocol ', protocol].join(''));\n      }\n\n      var endpointUrl = url.parse(endpoint);\n\n      addr = endpointUrl.hostname + ':' + endpointUrl.port;\n\n      brokerProfile.host = endpointUrl.hostname;\n      brokerProfile.port = endpointUrl.port;\n    } else {\n      addr = brokerProfile.host + ':' + brokerProfile.port;\n    }\n    assert(brokerProfile.host && brokerProfile.port, 'kafka host or port is empty');\n\n    self.brokerProfiles[addr] = brokerProfile;\n    self.brokerProfiles[addr].id = key;\n  });\n};\n\nClient.prototype.close = function (cb) {\n  this.closeBrokers(this.brokers);\n  this.closeBrokers(this.longpollingBrokers);\n  this.zk.close();\n  cb && cb();\n};\n\nClient.prototype.closeBrokers = function (brokers) {\n  _.each(brokers, function (broker) {\n    broker.socket.closing = true;\n    broker.socket.end();\n  });\n};\n\nClient.prototype.sendFetchRequest = function (consumer, payloads, fetchMaxWaitMs, fetchMinBytes, maxTickMessages) {\n  var self = this;\n  var encoder = protocol.encodeFetchRequest(fetchMaxWaitMs, fetchMinBytes);\n  var decoder = protocol.decodeFetchResponse(function (err, type, message) {\n    if (err) {\n      if (err.message === 'OffsetOutOfRange') {\n        return consumer.emit('offsetOutOfRange', err);\n      } else if (err.message === 'NotLeaderForPartition' || err.message === 'UnknownTopicOrPartition') {\n        return self.emit('brokersChanged');\n      }\n\n      return consumer.emit('error', err);\n    }\n\n    var encoding = consumer.options.encoding;\n\n    if (type === 'message') {\n      if (encoding !== 'buffer' && message.value) {\n        message.value = message.value.toString(encoding);\n      }\n\n      consumer.emit('message', message);\n    } else {\n      consumer.emit('done', message);\n    }\n  }, maxTickMessages);\n\n  this.send(payloads, encoder, decoder, function (err) {\n    if (err) {\n      Array.prototype.unshift.call(arguments, 'error');\n      consumer.emit.apply(consumer, arguments);\n    }\n  });\n};\n\nClient.prototype.sendProduceRequest = function (payloads, requireAcks, ackTimeoutMs, cb) {\n  var encoder = protocol.encodeProduceRequest(requireAcks, ackTimeoutMs);\n  var decoder = protocol.decodeProduceResponse;\n  var self = this;\n\n  decoder.requireAcks = requireAcks;\n\n  async.each(payloads, buildRequest, function (err) {\n    if (err) return cb(err);\n    self.send(payloads, encoder, decoder, function (err, result) {\n      if (err) {\n        if (err.message === 'NotLeaderForPartition') {\n          self.emit('brokersChanged');\n        }\n        cb(err);\n      } else {\n        cb(null, result);\n      }\n    });\n  });\n\n  function buildRequest (payload, cb) {\n    var attributes = payload.attributes;\n    var codec = getCodec(attributes);\n\n    if (!codec) return cb();\n\n    var innerSet = encodeMessageSet(payload.messages);\n    codec.encode(innerSet, function (err, message) {\n      if (err) return cb(err);\n      payload.messages = [ new Message(0, attributes, '', message) ];\n      cb();\n    });\n  }\n};\n\nClient.prototype.sendOffsetCommitRequest = function (group, payloads, cb) {\n  var encoder = protocol.encodeOffsetCommitRequest(group);\n  var decoder = protocol.decodeOffsetCommitResponse;\n  this.send(payloads, encoder, decoder, cb);\n};\n\nClient.prototype.sendOffsetCommitV2Request = function (group, generationId, memberId, payloads, cb) {\n  var encoder = protocol.encodeOffsetCommitV2Request;\n  var decoder = protocol.decodeOffsetCommitResponse;\n  this.sendGroupRequest(encoder, decoder, arguments);\n};\n\nClient.prototype.sendOffsetFetchV1Request = function (group, payloads, cb) {\n  var encoder = protocol.encodeOffsetFetchV1Request;\n  var decoder = protocol.decodeOffsetFetchV1Response;\n  this.sendGroupRequest(encoder, decoder, arguments);\n};\n\nClient.prototype.sendOffsetFetchRequest = function (group, payloads, cb) {\n  var encoder = protocol.encodeOffsetFetchRequest(group);\n  var decoder = protocol.decodeOffsetFetchResponse;\n  this.send(payloads, encoder, decoder, cb);\n};\n\nClient.prototype.sendOffsetRequest = function (payloads, cb) {\n  var encoder = protocol.encodeOffsetRequest;\n  var decoder = protocol.decodeOffsetResponse;\n  this.send(payloads, encoder, decoder, cb);\n};\n\nClient.prototype.sendGroupRequest = function (encode, decode, requestArgs) {\n  requestArgs = _.values(requestArgs);\n  var cb = requestArgs.pop();\n  var correlationId = this.nextId();\n\n  requestArgs.unshift(this.clientId, correlationId);\n\n  var request = encode.apply(null, requestArgs);\n  var broker = this.brokerForLeader(this.coordinatorId);\n\n  if (!broker || !broker.socket || broker.socket.error || broker.socket.destroyed) {\n    return cb(new errors.BrokerNotAvailableError('Broker not available'));\n  }\n\n  this.queueCallback(broker.socket, correlationId, [decode, cb]);\n  broker.write(request);\n};\n\nClient.prototype.sendGroupCoordinatorRequest = function (groupId, cb) {\n  this.sendGroupRequest(protocol.encodeGroupCoordinatorRequest, protocol.decodeGroupCoordinatorResponse, arguments);\n};\n\nClient.prototype.sendJoinGroupRequest = function (groupId, memberId, sessionTimeout, groupProtocol, cb) {\n  this.sendGroupRequest(protocol.encodeJoinGroupRequest, protocol.decodeJoinGroupResponse, arguments);\n};\n\nClient.prototype.sendSyncGroupRequest = function (groupId, generationId, memberId, groupAssignment, cb) {\n  this.sendGroupRequest(protocol.encodeSyncGroupRequest, protocol.decodeSyncGroupResponse, arguments);\n};\n\nClient.prototype.sendHeartbeatRequest = function (groupId, generationId, memberId, cb) {\n  this.sendGroupRequest(protocol.encodeGroupHeartbeat, protocol.decodeGroupHeartbeat, arguments);\n};\n\nClient.prototype.sendLeaveGroupRequest = function (groupId, memberId, cb) {\n  this.sendGroupRequest(protocol.encodeLeaveGroupRequest, protocol.decodeLeaveGroupResponse, arguments);\n};\n\n/*\n *  Helper method\n *  topic in paylods may send to different broker, so we cache data util all request came back\n */\nfunction wrap (payloads, cb) {\n  var out = {};\n  var count = Object.keys(payloads).length;\n\n  return function (err, data) {\n    // data: { topicName1: {}, topicName2: {} }\n    if (err) return cb && cb(err);\n    _.merge(out, data);\n    count -= 1;\n    // Waiting for all request return\n    if (count !== 0) return;\n    cb && cb(null, out);\n  };\n}\n\n/**\n * Fetches metadata information for a topic\n * This includes an array containing a each zookeeper node, their nodeId, host name, and port. As well as an object\n * containing the topic name, partition, leader number, replica count, and in sync replicas per partition.\n *\n * @param {Array} topics An array of topics to load the metadata for\n * @param {Client~loadMetadataForTopicsCallback} cb Function to call once all metadata is loaded\n */\nClient.prototype.loadMetadataForTopics = function (topics, cb) {\n  var correlationId = this.nextId();\n  var request = protocol.encodeMetadataRequest(this.clientId, correlationId, topics);\n  var broker = this.brokerForLeader();\n\n  if (!broker || !broker.socket || broker.socket.error || broker.socket.destroyed) {\n    return cb(new errors.BrokerNotAvailableError('Broker not available'));\n  }\n\n  this.queueCallback(broker.socket, correlationId, [protocol.decodeMetadataResponse, cb]);\n  broker.write(request);\n};\n\nClient.prototype.createTopics = function (topics, isAsync, cb) {\n  topics = typeof topics === 'string' ? [topics] : topics;\n\n  if (typeof isAsync === 'function' && typeof cb === 'undefined') {\n    cb = isAsync;\n    isAsync = true;\n  }\n\n  try {\n    validateKafkaTopics(topics);\n  } catch (e) {\n    if (isAsync) return cb(e);\n    throw e;\n  }\n\n  cb = _.once(cb);\n\n  const getTopicsFromKafka = (topics, callback) => {\n    this.loadMetadataForTopics(topics, function (error, resp) {\n      if (error) {\n        return callback(error);\n      }\n      callback(null, Object.keys(resp[1].metadata));\n    });\n  };\n\n  const operation = retry.operation({ minTimeout: 200, maxTimeout: 2000 });\n\n  operation.attempt(currentAttempt => {\n    logger.debug('create topics currentAttempt', currentAttempt);\n    getTopicsFromKafka(topics, function (error, kafkaTopics) {\n      if (error) {\n        if (operation.retry(error)) {\n          return;\n        }\n      }\n\n      logger.debug('kafka reported topics', kafkaTopics);\n      const left = _.difference(topics, kafkaTopics);\n      if (left.length === 0) {\n        logger.debug(`Topics created ${kafkaTopics}`);\n        return cb(null, kafkaTopics);\n      }\n\n      logger.debug(`Topics left ${left.join(', ')}`);\n      if (!operation.retry(new Error(`Topics not created ${left}`))) {\n        cb(operation.mainError());\n      }\n    });\n  });\n\n  if (!isAsync) {\n    cb(null);\n  }\n};\n\n/**\n * Checks to see if a given array of topics exists\n *\n * @param {Array} topics An array of topic names to check\n *\n * @param {Client~topicExistsCallback} cb A function to call after all topics have been checked\n */\nClient.prototype.topicExists = function (topics, cb) {\n  var notExistsTopics = [];\n  var self = this;\n\n  async.each(topics, checkZK, function (err) {\n    if (err) return cb(err);\n    if (notExistsTopics.length) return cb(new errors.TopicsNotExistError(notExistsTopics));\n    cb();\n  });\n\n  function checkZK (topic, cb) {\n    self.zk.topicExists(topic, function (err, existed, topic) {\n      if (err) return cb(err);\n      if (!existed) notExistsTopics.push(topic);\n      cb();\n    });\n  }\n};\n\nClient.prototype.addTopics = function (topics, cb) {\n  var self = this;\n  this.topicExists(topics, function (err) {\n    if (err) return cb(err);\n    self.loadMetadataForTopics(topics, function (err, resp) {\n      if (err) return cb(err);\n      self.updateMetadatas(resp);\n      cb(null, topics);\n    });\n  });\n};\n\nClient.prototype.nextId = function () {\n  if (this.correlationId >= MAX_INT32) {\n    this.correlationId = 0;\n  }\n  return this.correlationId++;\n};\n\nClient.prototype.nextSocketId = function () {\n  return this._socketId++;\n};\n\nClient.prototype.refreshBrokers = function () {\n  var self = this;\n  var validBrokers = Object.keys(this.brokerProfiles);\n\n  function closeDeadBrokers (brokers) {\n    var deadBrokerKeys = _.difference(Object.keys(brokers), validBrokers);\n    if (deadBrokerKeys.length) {\n      self.closeBrokers(deadBrokerKeys.map(function (key) {\n        var broker = brokers[key];\n        delete brokers[key];\n        return broker;\n      }));\n    }\n  }\n\n  closeDeadBrokers(this.brokers);\n  closeDeadBrokers(this.longpollingBrokers);\n};\n\nClient.prototype.refreshMetadata = function (topicNames, cb) {\n  var self = this;\n  if (!topicNames.length) return cb();\n  attemptRequestMetadata(topicNames, cb);\n\n  function attemptRequestMetadata (topics, cb) {\n    var operation = retry.operation({ minTimeout: 200, maxTimeout: 1000 });\n    operation.attempt(function (currentAttempt) {\n      logger.debug('refresh metadata currentAttempt', currentAttempt);\n      self.loadMetadataForTopics(topics, function (err, resp) {\n        err = err || resp[1].error;\n        if (operation.retry(err)) {\n          return;\n        }\n        if (err) {\n          logger.debug('refresh metadata error', err.message);\n          return cb(err);\n        }\n        self.updateMetadatas(resp);\n        cb();\n      });\n    });\n  }\n};\n\nClient.prototype.send = function (payloads, encoder, decoder, cb) {\n  var self = this;\n  var _payloads = payloads;\n  // payloads: [ [metadata exists], [metadata not exists] ]\n  payloads = this.checkMetadatas(payloads);\n  if (payloads[0].length && !payloads[1].length) {\n    this.sendToBroker(_.flatten(payloads), encoder, decoder, cb);\n    return;\n  }\n  if (payloads[1].length) {\n    var topicNames = payloads[1].map(function (p) { return p.topic; });\n    this.loadMetadataForTopics(topicNames, function (err, resp) {\n      if (err) {\n        return cb(err);\n      }\n\n      var error = resp[1].error;\n      if (error) {\n        return cb(error);\n      }\n\n      self.updateMetadatas(resp);\n      // check payloads again\n      payloads = self.checkMetadatas(_payloads);\n      if (payloads[1].length) {\n        return cb(new errors.BrokerNotAvailableError('Could not find the leader'));\n      }\n\n      self.sendToBroker(payloads[1].concat(payloads[0]), encoder, decoder, cb);\n    });\n  }\n};\n\nClient.prototype.sendToBroker = function (payloads, encoder, decoder, cb) {\n  var longpolling = encoder.name === 'encodeFetchRequest';\n  payloads = this.payloadsByLeader(payloads);\n  if (!longpolling) {\n    cb = wrap(payloads, cb);\n  }\n  for (var leader in payloads) {\n    if (!payloads.hasOwnProperty(leader)) {\n      continue;\n    }\n    var correlationId = this.nextId();\n    var request = encoder(this.clientId, correlationId, payloads[leader]);\n    var broker = this.brokerForLeader(leader, longpolling);\n    if (!broker || !broker.socket || broker.socket.error || broker.socket.closing || broker.socket.destroyed) {\n      return cb(new errors.BrokerNotAvailableError('Could not find the leader'), payloads[leader]);\n    }\n\n    if (longpolling) {\n      if (broker.socket.waiting) continue;\n      broker.socket.waiting = true;\n    }\n\n    if (decoder.requireAcks === 0) {\n      broker.writeAsync(request);\n      cb(null, { result: 'no ack' });\n    } else {\n      this.queueCallback(broker.socket, correlationId, [decoder, cb]);\n      broker.write(request);\n    }\n  }\n};\n\nClient.prototype.checkMetadatas = function (payloads) {\n  if (_.isEmpty(this.topicMetadata)) return [ [], payloads ];\n  // out: [ [metadata exists], [metadata not exists] ]\n  var out = [ [], [] ];\n  payloads.forEach(function (p) {\n    if (this.hasMetadata(p.topic, p.partition)) out[0].push(p);\n    else out[1].push(p);\n  }.bind(this));\n  return out;\n};\n\nClient.prototype.hasMetadata = function (topic, partition) {\n  var brokerMetadata = this.brokerMetadata;\n  var leader = this.leaderByPartition(topic, partition);\n\n  return (leader !== undefined) && brokerMetadata[leader];\n};\n\nClient.prototype.updateMetadatas = function (metadatas) {\n  // _.extend(this.brokerMetadata, metadatas[0])\n  _.extend(this.topicMetadata, metadatas[1].metadata);\n  for (var topic in this.topicMetadata) {\n    if (!this.topicMetadata.hasOwnProperty(topic)) {\n      continue;\n    }\n    this.topicPartitions[topic] = Object.keys(this.topicMetadata[topic]).map(function (val) {\n      return parseInt(val, 10);\n    });\n  }\n};\n\nClient.prototype.removeTopicMetadata = function (topics, cb) {\n  topics.forEach(function (t) {\n    if (this.topicMetadata[t]) delete this.topicMetadata[t];\n  }.bind(this));\n  cb(null, topics.length);\n};\n\nClient.prototype.payloadsByLeader = function (payloads) {\n  return payloads.reduce(function (out, p) {\n    var leader = this.leaderByPartition(p.topic, p.partition);\n    out[leader] = out[leader] || [];\n    out[leader].push(p);\n    return out;\n  }.bind(this), {});\n};\n\nClient.prototype.leaderByPartition = function (topic, partition) {\n  var topicMetadata = this.topicMetadata;\n  return topicMetadata[topic] && topicMetadata[topic][partition] && topicMetadata[topic][partition].leader;\n};\n\nClient.prototype.brokerForLeader = function (leader, longpolling) {\n  var addr;\n  var brokers = this.getBrokers(longpolling);\n  // If leader is not give, choose the first broker as leader\n  if (typeof leader === 'undefined') {\n    if (!_.isEmpty(brokers)) {\n      addr = Object.keys(brokers)[0];\n      return brokers[addr];\n    } else if (!_.isEmpty(this.brokerMetadata)) {\n      leader = Object.keys(this.brokerMetadata)[0];\n    } else {\n      return;\n    }\n  }\n\n  var broker = _.find(this.brokerProfiles, {id: leader});\n\n  if (!broker) {\n    return;\n  }\n\n  addr = broker.host + ':' + broker.port;\n\n  return brokers[addr] || this.setupBroker(broker.host, broker.port, longpolling, brokers);\n};\n\nClient.prototype.getBrokers = function (longpolling) {\n  return longpolling ? this.longpollingBrokers : this.brokers;\n};\n\nClient.prototype.setupBroker = function (host, port, longpolling, brokers) {\n  var brokerKey = host + ':' + port;\n  brokers[brokerKey] = this.createBroker(host, port, longpolling);\n  return brokers[brokerKey];\n};\n\nClient.prototype.createBroker = function (host, port, longpolling) {\n  var self = this;\n  var socket;\n  if (self.ssl) {\n    socket = tls.connect(port, host, self.sslOptions);\n  } else {\n    socket = net.createConnection(port, host);\n  }\n  socket.addr = host + ':' + port;\n  socket.host = host;\n  socket.port = port;\n  socket.socketId = this.nextSocketId();\n  if (longpolling) socket.longpolling = true;\n\n  socket.on('connect', function () {\n    var lastError = this.error;\n    this.error = null;\n    if (lastError) {\n      this.waiting = false;\n      self.emit('reconnect');\n    } else {\n      self.emit('connect');\n    }\n  });\n  socket.on('error', function (err) {\n    this.error = err;\n    self.emit('error', err);\n  });\n  socket.on('close', function (hadError) {\n    self.emit('close', this);\n    if (hadError && this.error) {\n      self.clearCallbackQueue(this, this.error);\n    } else {\n      self.clearCallbackQueue(this, new errors.BrokerNotAvailableError('Broker not available'));\n    }\n    retry(this);\n  });\n  socket.on('end', function () {\n    retry(this);\n  });\n  socket.buffer = new Buffer([]);\n  socket.on('data', function (data) {\n    this.buffer = Buffer.concat([this.buffer, data]);\n    self.handleReceivedData(this);\n  });\n  socket.setKeepAlive(true, 60000);\n\n  function retry (s) {\n    if (s.retrying || s.closing) return;\n    s.retrying = true;\n    s.retryTimer = setTimeout(function () {\n      if (s.closing) return;\n      self.reconnectBroker(s);\n    }, 1000);\n  }\n  return new BrokerWrapper(socket, this.noAckBatchOptions);\n};\n\nClient.prototype.reconnectBroker = function (oldSocket) {\n  oldSocket.retrying = false;\n  if (oldSocket.error) {\n    oldSocket.destroy();\n  }\n  var brokers = this.getBrokers(oldSocket.longpolling);\n  var newBroker = this.setupBroker(oldSocket.host, oldSocket.port, oldSocket.longpolling, brokers);\n  newBroker.socket.error = oldSocket.error;\n};\n\nClient.prototype.handleReceivedData = function (socket) {\n  var vars = Binary.parse(socket.buffer).word32bu('size').word32bu('correlationId').vars;\n  var size = vars.size + 4;\n  var correlationId = vars.correlationId;\n\n  if (socket.buffer.length >= size) {\n    var resp = socket.buffer.slice(0, size);\n    var handlers = this.unqueueCallback(socket, correlationId);\n\n    if (!handlers) return;\n    var decoder = handlers[0];\n    var cb = handlers[1];\n    var result = decoder(resp);\n    (result instanceof Error)\n      ? cb.call(this, result)\n      : cb.call(this, null, result);\n    socket.buffer = socket.buffer.slice(size);\n    if (socket.longpolling) socket.waiting = false;\n  } else { return; }\n\n  if (socket.buffer.length) {\n    setImmediate(function () { this.handleReceivedData(socket); }.bind(this));\n  }\n};\n\nClient.prototype.queueCallback = function (socket, id, data) {\n  var socketId = socket.socketId;\n  var queue;\n\n  if (this.cbqueue.hasOwnProperty(socketId)) {\n    queue = this.cbqueue[socketId];\n  } else {\n    queue = {};\n    this.cbqueue[socketId] = queue;\n  }\n\n  queue[id] = data;\n};\n\nClient.prototype.unqueueCallback = function (socket, id) {\n  var socketId = socket.socketId;\n\n  if (!this.cbqueue.hasOwnProperty(socketId)) {\n    return null;\n  }\n\n  var queue = this.cbqueue[socketId];\n  if (!queue.hasOwnProperty(id)) {\n    return null;\n  }\n\n  var result = queue[id];\n\n  // cleanup socket queue\n  delete queue[id];\n  if (!Object.keys(queue).length) {\n    delete this.cbqueue[socketId];\n  }\n\n  return result;\n};\n\nClient.prototype.clearCallbackQueue = function (socket, error) {\n  var socketId = socket.socketId;\n  var longpolling = socket.longpolling;\n\n  if (!this.cbqueue.hasOwnProperty(socketId)) {\n    return;\n  }\n\n  var queue = this.cbqueue[socketId];\n\n  if (!longpolling) {\n    Object.keys(queue).forEach(function (key) {\n      var handlers = queue[key];\n      var cb = handlers[1];\n      cb(error);\n    });\n  }\n  delete this.cbqueue[socketId];\n};\n\nmodule.exports = Client;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/wrapper/BrokerWrapper.js":"'use strict';\n\nvar BrokerReadable = require('./BrokerReadable');\nvar BrokerTransform = require('./BrokerTransform');\n\nvar BrokerWrapper = function (socket, noAckBatchOptions) {\n  this.socket = socket;\n\n  var self = this;\n  var readable = new BrokerReadable();\n  var transform = new BrokerTransform(noAckBatchOptions);\n\n  readable.pipe(transform);\n\n  transform.on('readable', function () {\n    var bulkMessage = null;\n    while (bulkMessage = transform.read()) { // eslint-disable-line no-cond-assign\n      self.socket.write(bulkMessage);\n    }\n  });\n\n  this.readableSocket = readable;\n};\n\nBrokerWrapper.prototype.write = function (buffer) {\n  this.socket.write(buffer);\n};\n\nBrokerWrapper.prototype.writeAsync = function (buffer) {\n  this.readableSocket.push(buffer);\n};\n\nmodule.exports = BrokerWrapper;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/wrapper/BrokerReadable.js":"'use strict';\n\nvar util = require('util');\nvar Readable = require('stream').Readable;\n\nvar BrokerReadable = function (options) {\n  Readable.call(this, options);\n};\n\nutil.inherits(BrokerReadable, Readable);\n\nBrokerReadable.prototype._read = function (size) {};\n\nmodule.exports = BrokerReadable;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/wrapper/BrokerTransform.js":"'use strict';\n\nvar util = require('util');\nvar Transform = require('stream').Transform;\nvar KafkaBuffer = require('../batch/KafkaBuffer');\n\nvar BrokerTransform = function (options) {\n  Transform.call(this, options);\n  this.noAckBatchSize = options ? options.noAckBatchSize : null;\n  this.noAckBatchAge = options ? options.noAckBatchAge : null;\n  this._KafkaBuffer = new KafkaBuffer(this.noAckBatchSize, this.noAckBatchAge);\n};\n\nutil.inherits(BrokerTransform, Transform);\n\nBrokerTransform.prototype._transform = function (chunk, enc, done) {\n  this._KafkaBuffer.addChunk(chunk, this._transformNext.bind(this));\n  done();\n};\n\nBrokerTransform.prototype._transformNext = function () {\n  this.push(this._KafkaBuffer.getBatch());\n  this._KafkaBuffer.truncateBatch();\n};\n\nmodule.exports = BrokerTransform;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/batch/KafkaBuffer.js":"'use strict';\n\nvar KafkaBuffer = function (batchSize, batchAge) {\n  this._batch_size = batchSize;\n  this._batch_age = batchAge;\n  this._batch_age_timer = null;\n  this._buffer = null;\n};\n\nKafkaBuffer.prototype.addChunk = function (buffer, callback) {\n  if (this._buffer == null) {\n    this._buffer = new Buffer(buffer);\n  } else {\n    this._buffer = Buffer.concat([this._buffer, buffer]);\n  }\n\n  if (typeof callback !== 'undefined' && callback != null) {\n    if (this._batch_size == null || this._batch_age == null ||\n      (this._buffer && (this._buffer.length > this._batch_size))) {\n      callback();\n    } else {\n      this._setupTimer(callback);\n    }\n  }\n};\n\nKafkaBuffer.prototype._setupTimer = function (callback) {\n  var self = this;\n\n  if (this._batch_age_timer != null) {\n    clearTimeout(this._batch_age_timer);\n  }\n\n  this._batch_age_timer = setTimeout(function () {\n    if (self._buffer && (self._buffer.length > 0)) {\n      callback();\n    }\n  }, this._batch_age);\n};\n\nKafkaBuffer.prototype.getBatch = function () {\n  return this._buffer;\n};\n\nKafkaBuffer.prototype.truncateBatch = function () {\n  this._buffer = null;\n};\n\nmodule.exports = KafkaBuffer;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/zookeeper.js":"'use strict';\n\nvar zookeeper = require('node-zookeeper-client');\nvar util = require('util');\nvar async = require('async');\nvar EventEmiter = require('events').EventEmitter;\nvar logger = require('./logging')('kafka-node:zookeeper');\n\n/**\n * Provides kafka specific helpers for talking with zookeeper\n *\n * @param {String} [connectionString='localhost:2181/kafka0.8'] A list of host:port for each zookeeper node and\n *      optionally a chroot path\n *\n * @constructor\n */\nvar Zookeeper = function (connectionString, options) {\n  this.client = zookeeper.createClient(connectionString, options);\n\n  var that = this;\n  this.client.on('connected', function () {\n    that.listBrokers();\n  });\n  this.client.on('disconnected', function () {\n    that.emit('disconnected');\n  });\n  this.client.connect();\n};\n\nutil.inherits(Zookeeper, EventEmiter);\n\nZookeeper.prototype.unregisterConsumer = function (groupId, consumerId, cb) {\n  var path = '/consumers/' + groupId + '/ids/' + consumerId;\n  var self = this;\n  logger.debug('unregister consumer node: %s', path);\n  async.waterfall([\n    function (callback) {\n      self.client.exists(path, callback);\n    },\n    function (stat, callback) {\n      if (stat) {\n        self.client.remove(path, function (error) {\n          if (error) {\n            logger.error(error);\n            return callback(error);\n          }\n          callback(null, true);\n        });\n      } else {\n        callback(null, false);\n      }\n    }\n  ], cb);\n};\n\nZookeeper.prototype.registerConsumer = function (groupId, consumerId, payloads, cb) {\n  var path = '/consumers/' + groupId;\n  var that = this;\n\n  async.series([\n    /* eslint-disable handle-callback-err */\n    function (callback) {\n      that.client.create(\n        path,\n        function (error, path) {\n          // simply carry on\n          callback();\n        });\n    },\n    function (callback) {\n      that.client.create(\n        path + '/ids',\n        function (error, path) {\n          // simply carry on\n          callback();\n        });\n    },\n    /* eslint-enable handle-callback-err */\n    function (callback) {\n      that.client.create(\n        path + '/ids/' + consumerId,\n        null,\n        null,\n        zookeeper.CreateMode.EPHEMERAL,\n        function (error, path) {\n          if (error) {\n            callback(error);\n          } else {\n            callback();\n          }\n        });\n    },\n    function (callback) {\n      var metadata = '{\"version\":1,\"subscription\":';\n      metadata += '{';\n      var sep = '';\n      payloads.map(function (p) {\n        metadata += sep + '\"' + p.topic + '\": 1';\n        sep = ', ';\n      });\n      metadata += '}';\n      var milliseconds = (new Date()).getTime();\n      metadata += ',\"pattern\":\"white_list\",\"timestamp\":\"' + milliseconds + '\"}';\n      that.client.setData(path + '/ids/' + consumerId, new Buffer(metadata), function (error, stat) {\n        if (error) {\n          callback(error);\n        } else {\n          logger.debug('Node: %s was created.', path + '/ids/' + consumerId);\n          cb();\n        }\n      });\n    }],\n    function (err) {\n      if (err) cb(err);\n      else cb();\n    });\n};\n\nZookeeper.prototype.getConsumersPerTopic = function (groupId, cb) {\n  var consumersPath = '/consumers/' + groupId + '/ids';\n  var brokerTopicsPath = '/brokers/topics';\n  var that = this;\n  var consumerPerTopicMap = new ZookeeperConsumerMappings();\n\n  async.series([\n    function (callback) {\n      that.client.getChildren(consumersPath, function (error, children, stats) {\n        if (error) {\n          callback(error);\n          return;\n        } else {\n          logger.debug('Children are: %j.', children);\n          async.each(children, function (consumer, cbb) {\n            var path = consumersPath + '/' + consumer;\n            that.client.getData(\n              path,\n              function (error, data) {\n                if (error) {\n                  cbb(error);\n                } else {\n                  try {\n                    var obj = JSON.parse(data.toString());\n                    // For each topic\n                    for (var topic in obj.subscription) {\n                      if (!obj.subscription.hasOwnProperty(topic)) {\n                        continue;\n                      }\n                      if (consumerPerTopicMap.topicConsumerMap[topic] == null) {\n                        consumerPerTopicMap.topicConsumerMap[topic] = [];\n                      }\n                      consumerPerTopicMap.topicConsumerMap[topic].push(consumer);\n\n                      if (consumerPerTopicMap.consumerTopicMap[consumer] == null) {\n                        consumerPerTopicMap.consumerTopicMap[consumer] = [];\n                      }\n                      consumerPerTopicMap.consumerTopicMap[consumer].push(topic);\n                    }\n\n                    cbb();\n                  } catch (e) {\n                    logger.error(e);\n                    cbb(new Error('Unable to assemble data'));\n                  }\n                }\n              }\n            );\n          }, function (err) {\n            if (err) {\n              callback(err);\n            } else {\n              callback();\n            }\n          });\n        }\n      });\n    },\n    function (callback) {\n      Object.keys(consumerPerTopicMap.topicConsumerMap).forEach(function (key) {\n        consumerPerTopicMap.topicConsumerMap[key] = consumerPerTopicMap.topicConsumerMap[key].sort();\n      });\n      callback();\n    },\n    function (callback) {\n      async.each(Object.keys(consumerPerTopicMap.topicConsumerMap), function (topic, cbb) {\n        var path = brokerTopicsPath + '/' + topic;\n        that.client.getData(\n          path,\n          function (error, data) {\n            if (error) {\n              cbb(error);\n            } else {\n              var obj = JSON.parse(data.toString());\n              // Get the topic partitions\n              var partitions = Object.keys(obj.partitions).map(function (partition) {\n                return partition;\n              });\n              consumerPerTopicMap.topicPartitionMap[topic] = partitions.sort(compareNumbers);\n              cbb();\n            }\n          }\n        );\n      }, function (err) {\n        if (err) {\n          callback(err);\n        } else {\n          callback();\n        }\n      });\n    }],\n    function (err) {\n      if (err) {\n        logger.debug(err);\n        cb(err);\n      } else cb(null, consumerPerTopicMap);\n    });\n};\n\nfunction compareNumbers (a, b) {\n  return a - b;\n}\n\nZookeeper.prototype.listPartitions = function (topic) {\n  var self = this;\n  var path = '/brokers/topics/' + topic + '/partitions';\n  this.client.getChildren(\n    path,\n    function () {\n      if (!self.closed) {\n        self.listPartitions(topic);\n      }\n    },\n    function (error, children) {\n      if (error) {\n        logger.error(error);\n        // Ignore NO_NODE error here #157\n        if (error.name !== 'NO_NODE') {\n          self.emit('error', error);\n        }\n      } else {\n        if (self.initPartitions) {\n          return self.emit('partitionsChanged');\n        }\n        self.initPartitions = true;\n      }\n    }\n  );\n};\n\nZookeeper.prototype.listBrokers = function (cb) {\n  var that = this;\n  var path = '/brokers/ids';\n  this.client.getChildren(\n    path,\n    function () {\n      that.listBrokers();\n    },\n    function (error, children) {\n      if (error) {\n        logger.debug('Failed to list children of node: %s due to: %s.', path, error);\n        that.emit('error', error);\n        return;\n      }\n\n      if (children.length) {\n        var brokers = {};\n        async.each(children, getBrokerDetail, function (err) {\n          if (err) {\n            that.emit('error', err);\n            return;\n          }\n          if (!that.inited) {\n            that.emit('init', brokers);\n            that.inited = true;\n          } else {\n            that.emit('brokersChanged', brokers);\n          }\n          cb && cb(brokers); // For test\n        });\n      } else {\n        if (that.inited) {\n          return that.emit('brokersChanged', {});\n        }\n        that.inited = true;\n        that.emit('init', {});\n      }\n\n      function getBrokerDetail (id, cb) {\n        var path = '/brokers/ids/' + id;\n        that.client.getData(path, function (err, data) {\n          if (err) return cb(err);\n          brokers[id] = JSON.parse(data.toString());\n          cb();\n        });\n      }\n    }\n  );\n};\n\nZookeeper.prototype.isConsumerRegistered = function (groupId, consumerId, callback) {\n  var path = '/consumers/' + groupId + '/ids/' + consumerId;\n  this.client.exists(path, function (error, stat) {\n    if (error) {\n      return callback(error);\n    }\n    callback(null, !!stat);\n  });\n};\n\nZookeeper.prototype.listConsumers = function (groupId) {\n  var that = this;\n  var path = '/consumers/' + groupId + '/ids';\n  this.client.getChildren(\n    path,\n    function () {\n      if (!that.closed) {\n        that.listConsumers(groupId);\n      }\n    },\n    function (error, children) {\n      if (error) {\n        logger.error(error);\n        // Ignore NO_NODE error here #157\n        if (error.name !== 'NO_NODE') {\n          that.emit('error', error);\n        }\n      } else {\n        if (that.listConsumersInitialized) {\n          that.emit('consumersChanged');\n        }\n      }\n      that.listConsumersInitialized = true;\n    }\n  );\n};\n\nZookeeper.prototype.topicExists = function (topic, cb, watch) {\n  var path = '/brokers/topics/' + topic;\n  var self = this;\n  this.client.exists(\n    path,\n    function (event) {\n      logger.debug('Got event: %s.', event);\n      if (watch) {\n        self.topicExists(topic, cb);\n      }\n    },\n    function (error, stat) {\n      if (error) return cb(error);\n      cb(null, !!stat, topic);\n    }\n  );\n};\n\nZookeeper.prototype.deletePartitionOwnership = function (groupId, topic, partition, cb) {\n  var path = '/consumers/' + groupId + '/owners/' + topic + '/' + partition;\n  this.client.remove(\n    path,\n    function (error) {\n      if (error) {\n        cb(error);\n      } else {\n        logger.debug('Removed partition ownership %s', path);\n        cb();\n      }\n    }\n  );\n};\n\nZookeeper.prototype.addPartitionOwnership = function (consumerId, groupId, topic, partition, cb) {\n  var path = '/consumers/' + groupId + '/owners/' + topic + '/' + partition;\n  var self = this;\n\n  async.series([\n    /* eslint-disable handle-callback-err */\n    function (callback) {\n      self.client.create(\n        '/consumers/' + groupId,\n        function (error, path) {\n          // simply carry on\n          callback();\n        });\n    },\n    function (callback) {\n      self.client.create(\n        '/consumers/' + groupId + '/owners',\n        function (error, path) {\n          // simply carry on\n          callback();\n        });\n    },\n    function (callback) {\n      self.client.create(\n        '/consumers/' + groupId + '/owners/' + topic,\n        function (error, path) {\n          // simply carry on\n          callback();\n        });\n    },\n    /* eslint-enable handle-callback-err */\n    function (callback) {\n      self.client.create(\n        path,\n        new Buffer(consumerId),\n        null,\n        zookeeper.CreateMode.EPHEMERAL,\n        function (error, path) {\n          if (error) {\n            callback(error);\n          } else callback();\n        });\n    },\n    function (callback) {\n      self.client.exists(path, null, function (error, stat) {\n        if (error) {\n          callback(error);\n        } else if (stat) {\n          logger.debug('Gained ownership of %s by %s.', path, consumerId);\n          callback();\n        } else {\n          callback(\"Path wasn't created\");\n        }\n      });\n    }],\n    function (err) {\n      if (err) cb(err);\n      else cb();\n    });\n};\n\nZookeeper.prototype.checkPartitionOwnership = function (consumerId, groupId, topic, partition, cb) {\n  var path = '/consumers/' + groupId + '/owners/' + topic + '/' + partition;\n  var self = this;\n\n  async.series([\n    function (callback) {\n      self.client.exists(path, null, function (error, stat) {\n        if (error) {\n          callback(error);\n        } else if (stat) {\n          callback();\n        } else {\n          callback(\"Path wasn't created\");\n        }\n      });\n    },\n    function (callback) {\n      self.client.getData(\n        path,\n        function (error, data) {\n          if (error) {\n            callback(error);\n          } else {\n            if (consumerId !== data.toString()) {\n              callback('Consumer not registered ' + consumerId);\n            } else callback();\n          }\n        }\n      );\n    }],\n    function (err) {\n      if (err) cb(err);\n      else cb();\n    });\n};\n\nZookeeper.prototype.close = function () {\n  this.closed = true;\n  this.client.close();\n};\n\nvar ZookeeperConsumerMappings = function () {\n  this.consumerTopicMap = {};\n  this.topicConsumerMap = {};\n  this.topicPartitionMap = {};\n};\n\nexports.Zookeeper = Zookeeper;\nexports.ZookeeperConsumerMappings = ZookeeperConsumerMappings;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/offset.js":"'use strict';\n\nvar util = require('util');\nvar async = require('async');\nvar events = require('events');\n\nvar Offset = function (client) {\n  var self = this;\n  this.client = client;\n  this.ready = this.client.ready;\n  this.client.on('ready', function () {\n    self.ready = true;\n    self.emit('ready');\n  });\n  this.client.once('connect', function () {\n    self.emit('connect');\n  });\n  this.client.on('error', function (err) {\n    self.emit('error', err);\n  });\n};\nutil.inherits(Offset, events.EventEmitter);\n\nOffset.prototype.fetch = function (payloads, cb) {\n  if (!this.ready) {\n    this.once('ready', () => this.fetch(payloads, cb));\n    return;\n  }\n  this.client.sendOffsetRequest(this.buildPayloads(payloads), cb);\n};\n\nOffset.prototype.buildPayloads = function (payloads) {\n  return payloads.map(function (p) {\n    p.partition = p.partition || 0;\n    p.time = p.time || Date.now();\n    p.maxNum = p.maxNum || 1;\n    p.metadata = 'm'; // metadata can be arbitrary\n    return p;\n  });\n};\n\nOffset.prototype.commit = function (groupId, payloads, cb) {\n  if (!this.ready) {\n    this.once('ready', () => this.commit(groupId, payloads, cb));\n    return;\n  }\n  this.client.sendOffsetCommitRequest(groupId, this.buildPayloads(payloads), cb);\n};\n\nOffset.prototype.fetchCommits = function (groupId, payloads, cb) {\n  if (!this.ready) {\n    this.once('ready', () => this.fetchCommits(groupId, payloads, cb));\n    return;\n  }\n  this.client.sendOffsetFetchRequest(groupId, this.buildPayloads(payloads), cb);\n};\n\nOffset.prototype.fetchLatestOffsets = function (topics, cb) {\n  fetchOffsets(this, topics, cb, -1);\n};\n\nOffset.prototype.fetchEarliestOffsets = function (topics, cb) {\n  fetchOffsets(this, topics, cb, -2);\n};\n\n// private helper\nfunction fetchOffsets (offset, topics, cb, when) {\n  if (!offset.ready) {\n    if (when === -1) {\n      offset.once('ready', () => offset.fetchLatestOffsets(topics, cb));\n    } else if (when === -2) {\n      offset.once('ready', () => offset.fetchEarliestOffsets(topics, cb));\n    }\n    return;\n  }\n  async.waterfall([\n    callback => {\n      offset.client.loadMetadataForTopics(topics, callback);\n    },\n    (topicsMetaData, callback) => {\n      var payloads = [];\n      var metaDatas = topicsMetaData[1].metadata;\n      Object.keys(metaDatas).forEach(function (topicName) {\n        var topic = metaDatas[topicName];\n        Object.keys(topic).forEach(function (partition) {\n          payloads.push({\n            topic: topicName,\n            partition: partition,\n            time: when\n          });\n        });\n      });\n      offset.fetch(payloads, callback);\n    },\n    function (results, callback) {\n      Object.keys(results).forEach(function (topicName) {\n        var topic = results[topicName];\n\n        Object.keys(topic).forEach(function (partitionName) {\n          topic[partitionName] = topic[partitionName][0];\n        });\n      });\n      callback(null, results);\n    }\n  ], cb);\n}\n\nmodule.exports = Offset;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/consumerGroupRecovery.js":"'use strict';\n\nconst retry = require('retry');\nconst logger = require('./logging')('kafka-node:ConsumerGroupRecovery');\nconst assert = require('assert');\n\nconst GroupCoordinatorNotAvailable = require('./errors/GroupCoordinatorNotAvailableError');\nconst NotCoordinatorForGroup = require('./errors/NotCoordinatorForGroupError');\nconst IllegalGeneration = require('./errors/IllegalGenerationError');\nconst GroupLoadInProgress = require('./errors/GroupLoadInProgressError');\nconst UnknownMemberId = require('./errors/UnknownMemberIdError');\nconst RebalanceInProgress = require('./errors/RebalanceInProgressError');\nconst HeartbeatTimeout = require('./errors/HeartbeatTimeoutError');\nconst BrokerNotAvailableError = require('./errors').BrokerNotAvailableError;\n\nconst recoverableErrors = [\n  {\n    errors: [GroupCoordinatorNotAvailable, IllegalGeneration, GroupLoadInProgress, RebalanceInProgress, HeartbeatTimeout]\n  },\n  {\n    errors: [NotCoordinatorForGroup, BrokerNotAvailableError],\n    handler: function () {\n      delete this.client.coordinatorId;\n    }\n  },\n  {\n    errors: [UnknownMemberId],\n    handler: function () {\n      this.memberId = null;\n    }\n  }\n];\n\nfunction isErrorInstanceOf (error, errors) {\n  return errors.some(function (errorClass) {\n    return error instanceof errorClass;\n  });\n}\n\nfunction ConsumerGroupRecovery (consumerGroup) {\n  this.consumerGroup = consumerGroup;\n  this.options = consumerGroup.options;\n}\n\nConsumerGroupRecovery.prototype.tryToRecoverFrom = function (error, source) {\n  this.consumerGroup.ready = false;\n  this.consumerGroup.stopHeartbeats();\n\n  var retryTimeout = false;\n  var retry = recoverableErrors.some(function (recoverableItem) {\n    if (isErrorInstanceOf(error, recoverableItem.errors)) {\n      recoverableItem.handler && recoverableItem.handler.call(this.consumerGroup, error);\n      return true;\n    }\n    return false;\n  }, this);\n\n  if (retry) {\n    retryTimeout = this.getRetryTimeout(error);\n  }\n\n  if (retry && retryTimeout) {\n    logger.debug('RECOVERY from %s: %s retrying in %s ms', source, this.consumerGroup.client.clientId, retryTimeout, error);\n    this.consumerGroup.scheduleReconnect(retryTimeout);\n  } else {\n    this.consumerGroup.emit('error', error);\n  }\n  this.lastError = error;\n};\n\nConsumerGroupRecovery.prototype.clearError = function () {\n  this.lastError = null;\n};\n\nConsumerGroupRecovery.prototype.getRetryTimeout = function (error) {\n  assert(error);\n  if (!this._timeouts) {\n    this._timeouts = retry.timeouts({\n      retries: this.options.retries,\n      factor: this.options.retryFactor,\n      minTimeout: this.options.retryMinTimeout\n    });\n  }\n\n  if (this._retryIndex == null || this.lastError == null ||\n      error.errorCode !== this.lastError.errorCode) {\n    this._retryIndex = 0;\n  }\n\n  var index = this._retryIndex++;\n  if (index >= this._timeouts.length) {\n    return false;\n  }\n  return this._timeouts[index];\n};\n\nmodule.exports = ConsumerGroupRecovery;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/consumerGroupHeartbeat.js":"'use strict';\n\nconst HeartbeatTimeoutError = require('./errors/HeartbeatTimeoutError');\nconst logger = require('./logging')('kafka-node:ConsumerGroupHeartbeat');\n\nmodule.exports = class Heartbeat {\n  constructor (client, handler) {\n    this.client = client;\n    this.handler = handler;\n    this.pending = true;\n  }\n\n  send (groupId, generationId, memberId) {\n    this.client.sendHeartbeatRequest(groupId, generationId, memberId, (error) => {\n      if (this.canceled) {\n        logger.debug('heartbeat yielded after being canceled', error);\n        return;\n      }\n      this.pending = false;\n      this.handler(error);\n    });\n  }\n\n  verifyResolved () {\n    if (this.pending) {\n      this.canceled = true;\n      this.pending = false;\n      this.handler(new HeartbeatTimeoutError('Heartbeat timed out'));\n      return false;\n    }\n    return true;\n  }\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/assignment/index.js":"'use strict';\n\nmodule.exports = {\n  'roundrobin': require('./roundrobin'),\n  'range': require('./range')\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/assignment/roundrobin.js":"'use strict';\n\nvar _ = require('lodash');\nvar groupPartitionsByTopic = require('../utils').groupPartitionsByTopic;\nvar logger = require('../logging')('kafka-node:Roundrobin');\nvar VERSION = 0;\n\nfunction assignRoundRobin (topicPartition, groupMembers, callback) {\n  logger.debug('topicPartition: %j', topicPartition);\n  logger.debug('groupMembers: %j', groupMembers);\n  var _members = _(groupMembers).map('id');\n  var members = _members.value().sort();\n  logger.debug('members', members);\n  var assignment = _members.reduce(function (obj, id) {\n    obj[id] = [];\n    return obj;\n  }, {});\n\n  var subscriberMap = groupMembers.reduce(function (subscribers, member) {\n    subscribers[member.id] = member.subscription;\n    return subscribers;\n  }, {});\n\n  logger.debug('subscribers', subscriberMap);\n\n  // layout topic/partitions pairs into a list\n  var topicPartitionList = _(topicPartition).map(function (partitions, topic) {\n    return partitions.map(function (partition) {\n      return {\n        topic: topic,\n        partition: partition\n      };\n    });\n  }).flatten().value();\n  logger.debug('round robin on topic partition pairs: ', topicPartitionList);\n\n  var assigner = cycle(members);\n\n  topicPartitionList.forEach(function (tp) {\n    var topic = tp.topic;\n    while (!_.includes(subscriberMap[assigner.peek()], topic)) {\n      assigner.next();\n    }\n    assignment[assigner.next()].push(tp);\n  });\n\n  var ret = _.map(assignment, function (value, key) {\n    var ret = {};\n    ret.memberId = key;\n    ret.topicPartitions = groupPartitionsByTopic(value);\n    ret.version = VERSION;\n    return ret;\n  });\n\n  callback(null, ret);\n}\n\nfunction cycle (arr) {\n  var index = -1;\n  var len = arr.length;\n  return {\n    peek: function () {\n      return arr[(index + 1) % len];\n    },\n    next: function () {\n      index = ++index % len;\n      return arr[index];\n    }\n  };\n}\n\nmodule.exports = {\n  assign: assignRoundRobin,\n  name: 'roundrobin',\n  version: VERSION\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/assignment/range.js":"'use strict';\n\nconst logger = require('../logging')('kafka-node:Range');\nconst VERSION = 0;\nconst _ = require('lodash');\nconst groupPartitionsByTopic = require('../utils').groupPartitionsByTopic;\n\nfunction assignRange (topicPartition, groupMembers, callback) {\n  logger.debug('topicPartition: %j', topicPartition);\n  var assignment = _(groupMembers).map('id').reduce(function (obj, id) {\n    obj[id] = [];\n    return obj;\n  }, {});\n\n  const topicMemberMap = topicToMemberMap(groupMembers);\n  for (var topic in topicMemberMap) {\n    if (!topicMemberMap.hasOwnProperty(topic)) {\n      continue;\n    }\n    logger.debug('For topic %s', topic);\n    topicMemberMap[topic].sort();\n    logger.debug('   members: ', topicMemberMap[topic]);\n\n    var numberOfPartitionsForTopic = topicPartition[topic].length;\n    logger.debug('   numberOfPartitionsForTopic', numberOfPartitionsForTopic);\n\n    var numberOfMembersForTopic = topicMemberMap[topic].length;\n    logger.debug('   numberOfMembersForTopic', numberOfMembersForTopic);\n\n    var numberPartitionsPerMember = Math.floor(numberOfPartitionsForTopic / numberOfMembersForTopic);\n    logger.debug('   numberPartitionsPerMember', numberPartitionsPerMember);\n\n    var membersWithExtraPartition = numberOfPartitionsForTopic % numberOfMembersForTopic;\n    var topicPartitionList = createTopicPartitionArray(topic, numberOfPartitionsForTopic);\n\n    for (var i = 0, n = numberOfMembersForTopic; i < n; i++) {\n      var start = numberPartitionsPerMember * i + Math.min(i, membersWithExtraPartition);\n      var length = numberPartitionsPerMember + (i + 1 > membersWithExtraPartition ? 0 : 1);\n      var assignedTopicPartitions = assignment[topicMemberMap[topic][i]];\n      assignedTopicPartitions.push.apply(assignedTopicPartitions, topicPartitionList.slice(start, start + length));\n    }\n  }\n\n  logger.debug(assignment);\n\n  callback(null, convertToAssignmentList(assignment, VERSION));\n}\n\nfunction convertToAssignmentList (assignment, version) {\n  return _.map(assignment, function (value, key) {\n    return {\n      memberId: key,\n      topicPartitions: groupPartitionsByTopic(value),\n      version: version\n    };\n  });\n}\n\nfunction createTopicPartitionArray (topic, numberOfPartitions) {\n  return _.times(numberOfPartitions, function (n) {\n    return {\n      topic: topic,\n      partition: n\n    };\n  });\n}\n\nfunction topicToMemberMap (groupMembers) {\n  return groupMembers.reduce(function (result, member) {\n    member.subscription.forEach(function (topic) {\n      if (topic in result) {\n        result[topic].push(member.id);\n      } else {\n        result[topic] = [member.id];\n      }\n    });\n    return result;\n  }, {});\n}\n\nmodule.exports = {\n  assign: assignRange,\n  name: 'range',\n  version: VERSION\n};\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/consumer.js":"'use strict';\n\nvar util = require('util');\nvar _ = require('lodash');\nvar events = require('events');\nvar logger = require('./logging')('kafka-node:Consumer');\nvar utils = require('./utils');\n\nvar DEFAULTS = {\n  groupId: 'kafka-node-group',\n  // Auto commit config\n  autoCommit: true,\n  autoCommitIntervalMs: 5000,\n  // Fetch message config\n  fetchMaxWaitMs: 100,\n  fetchMinBytes: 1,\n  fetchMaxBytes: 1024 * 1024,\n  fromOffset: false,\n  encoding: 'utf8'\n};\n\nvar nextId = (function () {\n  var id = 0;\n  return function () {\n    return id++;\n  };\n})();\n\nvar Consumer = function (client, topics, options) {\n  if (!topics) {\n    throw new Error('Must have payloads');\n  }\n\n  utils.validateTopics(topics);\n\n  this.fetchCount = 0;\n  this.client = client;\n  this.options = _.defaults((options || {}), DEFAULTS);\n  this.ready = false;\n  this.paused = this.options.paused;\n  this.id = nextId();\n  this.payloads = this.buildPayloads(topics);\n  this.connect();\n  this.encoding = this.options.encoding;\n\n  if (this.options.groupId) {\n    utils.validateConfig('options.groupId', this.options.groupId);\n  }\n};\nutil.inherits(Consumer, events.EventEmitter);\n\nConsumer.prototype.buildPayloads = function (payloads) {\n  var self = this;\n  return payloads.map(function (p) {\n    if (typeof p !== 'object') p = { topic: p };\n    p.partition = p.partition || 0;\n    p.offset = p.offset || 0;\n    p.maxBytes = self.options.fetchMaxBytes;\n    p.metadata = 'm'; // metadata can be arbitrary\n    return p;\n  });\n};\n\nConsumer.prototype.connect = function () {\n  var self = this;\n  // Client already exists\n  this.ready = this.client.ready;\n  if (this.ready) this.init();\n\n  this.client.on('ready', function () {\n    logger.debug('consumer ready');\n    if (!self.ready) self.init();\n    self.ready = true;\n  });\n\n  this.client.on('error', function (err) {\n    logger.error('client error %s', err.message);\n    self.emit('error', err);\n  });\n\n  this.client.on('close', function () {\n    logger.debug('connection closed');\n  });\n\n  this.client.on('brokersChanged', function () {\n    var topicNames = self.payloads.map(function (p) {\n      return p.topic;\n    });\n\n    this.refreshMetadata(topicNames, function (err) {\n      if (err) return self.emit('error', err);\n      self.fetch();\n    });\n  });\n  // 'done' will be emit when a message fetch request complete\n  this.on('done', function (topics) {\n    self.updateOffsets(topics);\n    setImmediate(function () {\n      self.fetch();\n    });\n  });\n};\n\nConsumer.prototype.init = function () {\n  if (!this.payloads.length) {\n    return;\n  }\n\n  var self = this;\n  var topics = self.payloads.map(function (p) { return p.topic; });\n\n  self.client.topicExists(topics, function (err) {\n    if (err) {\n      return self.emit('error', err);\n    }\n\n    if (self.options.fromOffset) {\n      return self.fetch();\n    }\n\n    self.fetchOffset(self.payloads, function (err, topics) {\n      if (err) {\n        return self.emit('error', err);\n      }\n\n      self.updateOffsets(topics, true);\n      self.fetch();\n    });\n  });\n};\n\n/*\n * Update offset info in current payloads\n * @param {Object} Topic-partition-offset\n * @param {Boolean} Don't commit when initing consumer\n */\nConsumer.prototype.updateOffsets = function (topics, initing) {\n  this.payloads.forEach(function (p) {\n    if (!_.isEmpty(topics[p.topic]) && topics[p.topic][p.partition] !== undefined) {\n      var offset = topics[p.topic][p.partition];\n      if (offset === -1) offset = 0;\n      if (!initing) p.offset = offset + 1;\n      else p.offset = offset;\n    }\n  });\n\n  if (this.options.autoCommit && !initing) {\n    this.autoCommit(false, function (err) {\n      err && logger.debug('auto commit offset', err);\n    });\n  }\n};\n\nfunction autoCommit (force, cb) {\n  if (arguments.length === 1) {\n    cb = force;\n    force = false;\n  }\n\n  if (this.committing && !force) return cb(null, 'Offset committing');\n\n  this.committing = true;\n  setTimeout(function () {\n    this.committing = false;\n  }.bind(this), this.options.autoCommitIntervalMs);\n\n  var payloads = this.payloads;\n  if (this.pausedPayloads) payloads = payloads.concat(this.pausedPayloads);\n\n  var commits = payloads.filter(function (p) { return p.offset !== 0; });\n  if (commits.length) {\n    this.client.sendOffsetCommitRequest(this.options.groupId, commits, cb);\n  } else {\n    cb(null, 'Nothing to be committed');\n  }\n}\nConsumer.prototype.commit = Consumer.prototype.autoCommit = autoCommit;\n\nConsumer.prototype.fetch = function () {\n  if (!this.ready || this.paused) return;\n  this.client.sendFetchRequest(this, this.payloads, this.options.fetchMaxWaitMs, this.options.fetchMinBytes);\n};\n\nConsumer.prototype.fetchOffset = function (payloads, cb) {\n  this.client.sendOffsetFetchRequest(this.options.groupId, payloads, cb);\n};\n\nConsumer.prototype.addTopics = function (topics, cb, fromOffset) {\n  fromOffset = !!fromOffset;\n  var self = this;\n  if (!this.ready) {\n    setTimeout(function () {\n      self.addTopics(topics, cb, fromOffset);\n    }\n    , 100);\n    return;\n  }\n\n  // The default is that the topics is a string array of topic names\n  var topicNames = topics;\n\n  // If the topics is actually an object and not string we assume it is an array of payloads\n  if (typeof topics[0] === 'object') {\n    topicNames = topics.map(function (p) { return p.topic; });\n  }\n\n  this.client.addTopics(\n    topicNames,\n    function (err, added) {\n      if (err) return cb && cb(err, added);\n\n      var payloads = self.buildPayloads(topics);\n      var reFetch = !self.payloads.length;\n\n      if (fromOffset) {\n        payloads.forEach(function (p) {\n          self.payloads.push(p);\n        });\n        if (reFetch) self.fetch();\n        cb && cb(null, added);\n        return;\n      }\n\n      // update offset of topics that will be added\n      self.fetchOffset(payloads, function (err, offsets) {\n        if (err) return cb(err);\n        payloads.forEach(function (p) {\n          var offset = offsets[p.topic][p.partition];\n          if (offset === -1) offset = 0;\n          p.offset = offset;\n          self.payloads.push(p);\n        });\n        if (reFetch) self.fetch();\n        cb && cb(null, added);\n      });\n    }\n  );\n};\n\nConsumer.prototype.removeTopics = function (topics, cb) {\n  topics = typeof topics === 'string' ? [topics] : topics;\n  this.payloads = this.payloads.filter(function (p) {\n    return !~topics.indexOf(p.topic);\n  });\n\n  this.client.removeTopicMetadata(topics, cb);\n};\n\nConsumer.prototype.close = function (force, cb) {\n  this.ready = false;\n  if (typeof force === 'function') {\n    cb = force;\n    force = false;\n  }\n\n  if (force) {\n    this.commit(force, function (err) {\n      if (err) {\n        return cb(err);\n      }\n      this.client.close(cb);\n    }.bind(this));\n  } else {\n    this.client.close(cb);\n  }\n};\n\nConsumer.prototype.setOffset = function (topic, partition, offset) {\n  this.payloads.every(function (p) {\n    if (p.topic === topic && p.partition == partition) { // eslint-disable-line eqeqeq\n      p.offset = offset;\n      return false;\n    }\n    return true;\n  });\n};\n\nConsumer.prototype.pause = function () {\n  this.paused = true;\n};\n\nConsumer.prototype.resume = function () {\n  this.paused = false;\n  this.fetch();\n};\n\nConsumer.prototype.pauseTopics = function (topics) {\n  if (!this.pausedPayloads) this.pausedPayloads = [];\n  pauseOrResume(this.payloads, this.pausedPayloads, topics);\n};\n\nConsumer.prototype.resumeTopics = function (topics) {\n  if (!this.pausedPayloads) this.pausedPayloads = [];\n  var reFetch = !this.payloads.length;\n  pauseOrResume(this.pausedPayloads, this.payloads, topics);\n  reFetch = reFetch && this.payloads.length;\n  if (reFetch) this.fetch();\n};\n\nfunction pauseOrResume (payloads, nextPayloads, topics) {\n  if (!topics || !topics.length) return;\n\n  for (var i = 0, j = 0, l = payloads.length; j < l; i++, j++) {\n    if (isInTopics(payloads[i])) {\n      nextPayloads.push(\n        payloads.splice(i, 1)[0]\n      );\n      i--;\n    }\n  }\n\n  function isInTopics (p) {\n    return topics.some(function (topic) {\n      if (typeof topic === 'string') {\n        return p.topic === topic;\n      } else {\n        return p.topic === topic.topic && p.partition === topic.partition;\n      }\n    });\n  }\n}\n\nmodule.exports = Consumer;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/producer.js":"'use strict';\n\nvar util = require('util');\nvar BaseProducer = require('./baseProducer');\n\n/** @inheritdoc */\nfunction Producer (client, options, customPartitioner) {\n  BaseProducer.call(this, client, options, BaseProducer.PARTITIONER_TYPES.default, customPartitioner);\n}\n\nutil.inherits(Producer, BaseProducer);\n\nProducer.PARTITIONER_TYPES = BaseProducer.PARTITIONER_TYPES;\n\nmodule.exports = Producer;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/logging.js":"'use strict';\n\nconst logging = require('./lib/logging');\n\nexports.setLoggerProvider = logging.setLoggerProvider;\n","/home/travis/build/npmtest/node-npmtest-kafka-node/node_modules/kafka-node/lib/consumerGroupMigrator.js":"'use strict';\n\nconst assert = require('assert');\nconst util = require('util');\nconst async = require('async');\nconst logger = require('./logging')('kafka-node:ConsumerGroupMigrator');\nconst zookeeper = require('node-zookeeper-client');\nconst _ = require('lodash');\nconst EventEmitter = require('events').EventEmitter;\nconst NUMER_OF_TIMES_TO_VERIFY = 4;\nconst VERIFY_WAIT_TIME_MS = 1500;\n\nfunction ConsumerGroupMigrator (consumerGroup) {\n  EventEmitter.call(this);\n  assert(consumerGroup);\n  const self = this;\n  this.consumerGroup = consumerGroup;\n  this.client = consumerGroup.client;\n  var verified = 0;\n\n  if (consumerGroup.options.migrateRolling) {\n    this.zk = zookeeper.createClient(consumerGroup.client.connectionString, {retries: 10});\n    this.zk.on('connected', function () {\n      self.filterByExistingZkTopics(function (error, topics) {\n        if (error) {\n          return self.emit('error', error);\n        }\n\n        if (topics.length) {\n          self.checkForOwnersAndListenForChange(topics);\n        } else {\n          logger.debug('No HLC topics exist in zookeeper.');\n          self.connectConsumerGroup();\n        }\n      });\n    });\n\n    this.on('noOwnersForTopics', function (topics) {\n      logger.debug('No owners for topics %s reported.', topics);\n      if (++verified <= NUMER_OF_TIMES_TO_VERIFY) {\n        logger.debug('%s verify %d of %d HLC has given up ownership by checking again in %d', self.client.clientId, verified,\n          NUMER_OF_TIMES_TO_VERIFY, VERIFY_WAIT_TIME_MS);\n\n        setTimeout(function () {\n          self.checkForOwners(topics);\n        }, VERIFY_WAIT_TIME_MS);\n      } else {\n        self.connectConsumerGroup();\n      }\n    });\n\n    this.on('topicOwnerChange', _.debounce(function (topics) {\n      verified = 0;\n      self.checkForOwnersAndListenForChange(topics);\n    }, 250));\n\n    this.zk.connect();\n  } else {\n    this.connectConsumerGroup();\n  }\n}\n\nutil.inherits(ConsumerGroupMigrator, EventEmitter);\n\nConsumerGroupMigrator.prototype.connectConsumerGroup = function () {\n  logger.debug('%s connecting consumer group', this.client.clientId);\n  const self = this;\n  if (this.client.ready) {\n    this.consumerGroup.connect();\n  } else {\n    this.client.once('ready', function () {\n      self.consumerGroup.connect();\n    });\n  }\n  this.zk && this.zk.close();\n};\n\nConsumerGroupMigrator.prototype.filterByExistingZkTopics = function (callback) {\n  const self = this;\n  const path = '/consumers/' + this.consumerGroup.options.groupId + '/owners/';\n\n  async.filter(this.consumerGroup.topics, function (topic, cb) {\n    const topicPath = path + topic;\n    logger.debug('%s checking zk path %s', self.client.clientId, topicPath);\n    self.zk.exists(topicPath, function (error, stat) {\n      if (error) {\n        return callback(error);\n      }\n      cb(stat);\n    });\n  }, function (result) {\n    callback(null, result);\n  });\n};\n\nConsumerGroupMigrator.prototype.checkForOwnersAndListenForChange = function (topics) {\n  this.checkForOwners(topics, true);\n};\n\nConsumerGroupMigrator.prototype.checkForOwners = function (topics, listenForChange) {\n  const self = this;\n  const path = '/consumers/' + this.consumerGroup.options.groupId + '/owners/';\n  var ownedPartitions = 0;\n\n  function topicWatcher (event) {\n    self.emit('topicOwnerChange', topics);\n  }\n\n  async.each(topics,\n    function (topic, callback) {\n      const args = [path + topic];\n\n      if (listenForChange) {\n        logger.debug('%s listening for changes in topic %s', self.client.clientId, topic);\n        args.push(topicWatcher);\n      }\n\n      args.push(function (error, children, stats) {\n        if (error) {\n          return callback(error);\n        }\n        ownedPartitions += children.length;\n        callback(null);\n      });\n\n      self.zk.getChildren.apply(self.zk, args);\n    },\n    function (error) {\n      if (error) {\n        return self.emit('error', error);\n      }\n      if (ownedPartitions === 0) {\n        self.emit('noOwnersForTopics', topics);\n      } else {\n        logger.debug('%s %d partitions are owned by old HLC... waiting...', self.client.clientId, ownedPartitions);\n      }\n    }\n  );\n};\n\nConsumerGroupMigrator.prototype.saveHighLevelConsumerOffsets = function (topicPartitionList, callback) {\n  const self = this;\n  this.client.sendOffsetFetchRequest(this.consumerGroup.options.groupId, topicPartitionList, function (error, results) {\n    logger.debug('sendOffsetFetchRequest response:', results, error);\n    if (error) {\n      return callback(error);\n    }\n    self.offsets = results;\n    callback(null);\n  });\n};\n\nConsumerGroupMigrator.prototype.getOffset = function (tp, defaultOffset) {\n  const offset = _.get(this.offsets, [tp.topic, tp.partition], defaultOffset);\n  if (offset === -1) {\n    return defaultOffset;\n  }\n  return offset;\n};\n\nmodule.exports = ConsumerGroupMigrator;\n"}